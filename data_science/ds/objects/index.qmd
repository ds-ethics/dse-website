---
title:  '{{< animate fadeInDown "Bias in AI: detection and mitigation"delay=.6s >}}'
subtitle: '{{< animate fadeInDown "Enable users to detect and mitigate bias, using the example of the COMPAS Recidivism Risk Score Data and Analysis Dataset. Users will be equiped with concrete strategies to first detect, and secondly mitigate bias"delay=.6s >}}'
author: ["Jorge Roa", "Carlo Greß", "Hannah Schweren"]
date: "2023-12-05"
categories: ["Advanced", "Bias in AI", "Mitigation"]
toc: true
draft: false
code-link: true
code-copy: true
title-block-banner: true
comments: false
image: images/dalle.png
include-in-header: meta.html 
format: html
filters:
   - lightbox
   - webr
lightbox: 
  match: auto
  effect: fade
  desc-position: left
  css-class: "lightwidth"
webr:
  packages: ['ggplot2', 'dplyr']
  show-startup-message: false
  show-header-message: false
jupyter: python3
---

<br>

# Introduction

This notebook offers a detailed guide that includes both code and explanations aimed at enabling users to identify and counteract bias within data, specifically using the COMPAS Recidivism Risk Score Data and Analysis Dataset as a case study. It provides users with practical strategies to first detect and then mitigate bias, laying a foundational approach for handling biases effectively in algorithmic processes. The tutorial is designed as an introductory step towards fostering an understanding of the biases that can infiltrate algorithms and promoting the development of ethical AI practices. This is particularly critical in contexts where algorithmic decisions intersect with policy-making, potentially influencing societal outcomes. Through this guide, users will not only learn to recognize biases but also implement measures to address these biases, thereby enhancing the fairness and integrity of AI systems in public and private sectors.

<br>

# Overview

The COMPAS dataset, used by an algorithm predicting recidivism risk, has become a key example in the study of algorithmic bias and fairness. It includes demographic and criminal history data. Analyses revealed racial disparities in risk assessments, with the algorithm tending to overestimate recidivism risk for Black defendants and underestimate it for White defendants.

This tutorial is divided into three parts:

1.- *Introduction to Bias Detection Metrics*: We will introduce different metrics to detect bias, providing a smooth introduction to the topic and helping users gain a better understanding of the issue.

2.- *Replication of Biased Output with a Feed Forward Neural Network*: In this step, we will replicate the biased output using a Feed Forward Neural Network. This hands-on exercise will provide users with practical experience in generating predictions and raise awarness for the biased output.

3.*-Mitigation of Detected Bias*: The grand finale and most important part of our tutorial! Users will learn effective strategies to mitigate the detected bias. This step is crucial for ethical deep learning, and the tutorial aims to equip users with essential skills dealing with biased results.

By completing this tutorial, users will acquire valuable skills for future data endeavors. It serves as a foundational step to train users and raise awareness of fairness issues in Deep Learning. 

<br>


## Background and Prerequisites

This tutorial is designed for users with a basic understanding of Python and Deep Learning. Users should have a foundational understanding of key concepts in machine learning and neural networks. Familiarity with Python is essential. Additionally, a grasp of linear algebra and calculus will be beneficial for understanding the mathematical underpinnings of deep learning algorithms.

- A solid understanding of model training is crucial, as well as knowledge of common machine learning libraries such as   `Keras` and `scikit-learn`. Users should also be aware of the ethical and policy considerations surrounding machine learning applications, particularly in relation to bias and fairness.

- Lastly, a conceptual understanding of how neural networks operate, including layers, activation functions, and backpropagation, will enhance the learning experience of the user. Overall, a basic background in machine learning fundamentals will help users to engage more effectively with our tutorial.

```{python packages}
#| message: false
#| warning: false
#| output: false

!pip install pandas numpy matplotlib
!pip install Aequitas
!pip install keras_tuner
!pip install aif360
!pip install BlackBoxAuditing
!pip install tensorflow
```

```{python libraries}
#| message: false
#| warning: false


# Data visualization
import matplotlib.pyplot as plt
import seaborn as sns
sns.set(style="white", palette="muted", color_codes=True, context="talk")
from IPython import display

# Data manipulation
import pandas as pd
import numpy as np
from tqdm import tqdm

# Aequitas library used to audit models for discrimination and bias
from aequitas.group import Group
from aequitas.bias import Bias
from aequitas.fairness import Fairness
from aequitas.plotting import Plot
import matplotlib.pyplot as plt
import warnings; warnings.simplefilter('ignore')

# Machine and deep learning libraries
import tensorflow as tf
from keras.layers import Input, Dense, Dropout
from keras.models import Model
from keras.optimizers import Adam
import keras_tuner as kt
from keras import Input, Model
from keras.layers import Dense, Dropout
from keras.optimizers import Adam
from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import accuracy_score, roc_auc_score
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix

# AI fairness library
from aif360.algorithms.preprocessing import DisparateImpactRemover
from aif360.datasets import StandardDataset as Dataset
from aif360.metrics import BinaryLabelDatasetMetric
from aif360.algorithms.postprocessing.reject_option_classification import RejectOptionClassification
from aif360.algorithms.preprocessing.reweighing import Reweighing
from collections import OrderedDict
from aif360.metrics import ClassificationMetric

```

# Data Description

In this tutorial we are working with the COMPAS Recidivism Risk Score Data and Analysis (Source: Pro Publica, https://www.propublica.org/datastore/dataset/compas-recidivism-risk-score-data-and-analysis) This dataset is a classical example for bias in machine learning. We specifically liked using this dataset as an example because it reveils the possible harmfull negative impact on real world decisions, that algorithms can have and the resulting policy responsibility.

The tabular dataset is used in U.S. court proceedings to evaluate the probability of a defendant reoffending. It is available in csv format for free and contains the following information (Source: https://mlr3fairness.mlr-org.com/reference/compas.html#pre-processing) :

* (integer) age : The age of defendants.

* (factor) c_charge_degree : The charge degree of defendants. F: Felony M: Misdemeanor

* (factor) race: The race of defendants.

* (factor) age_cat: The age category of defendants.

* (factor) score_text: The score category of defendants.

* (factor) sex: The sex of defendants.

* (integer) priors_count: The prior criminal records of defendants.

* (integer) days_b_screening_arrest: The count of days between screening date and (original) arrest date. If they are too far apart, that may indicate an error. If the value is negative, that indicate the screening date happened before the arrest date.

* (integer) decile_score: Indicate the risk of recidivism (Min=1, Max=10)

* (integer) is_recid: Binary variable indicate whether defendant is rearrested at any time.

* (factor) two_year_recid: Binary variable indicate whether defendant is rearrested at within two years.

* (numeric) length_of_stay: The count of days stay in jail.

In the course of the tutorial, we'll also work with a version of the COMPAS data, that was processed to work well with the aequitas package - this version of the dataset can be found in this Github repository: https://github.com/dssg/aequitas/tree/master/examples/data. Here, only a subset of the variables is considered, but it includes all important variables for demonstrating the package's benefits. It includes:

* (integer) entity_id: ID variable

* (integer) score: Risk score of defendants, binary

* (factor) label_value: Binary variable indicate whether defendant is rearrested

* (factor) race: The race of defendants.

* (factor) sex: The sex of defendants

* (factor) age_cat: The age category of defendants





------------------------------------------------------------------------

# References

------------------------------------------------------------------------


::: {.callout-tip}
## References

- Regean, Mary. 2021. "Understanding bias and fairness in AI system". [URL](https://towardsdatascience.com/understanding-bias-and-fairness-in-aisystems-6f7fbfe267f3)

- Anaconda, 2021 State of Data Science Report. [URL](https://know.anaconda.com/rs/387-XNW-688/images/Anaconda-2021-SODS-Report-Final.pdf)

- Clark, Andrew. September 19, 2022. "Top bias metrics and how they work". Monitaur. [URL](https://www.monitaur.ai/blog-posts/top-bias-metricsand-how-they-work)

- Feldman et al. „Certifying and Removing Disparate Impact“. Proceedings of the 21th ACM SIGKDD International Conference on KnowledgeDiscovery and Data Mining, ACM, 2015, S. 259–68. DOI.org (Crossref), [URL](https://doi.org/10.1145/2783258.2783311).

- Towards datascience, "AI Fairness — Explanation of Disparate Impact Remover". [URL](https://towardsdatascience.com/ai-fairness-explanation-of-disparate-impact-remover-ce0da59451f1)

- Lee, Nicol Turner, Paul Resnick, and Genie Barton. "Algorithmic bias detection and mitigation: Best practices and policies to reduce consumer harms." Brookings Institute: Washington, DC, USA 2 (2019)
:::

::: callout-note
Cite this page: Roa, J. (2023, April 16). *Understanding R Objects: Data Structures and Classes in R*. /. [URL](https://www.hertiecodingclub.com/learn/rstudio/rstudio101/)
:::


