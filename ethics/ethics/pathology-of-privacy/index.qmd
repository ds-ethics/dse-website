---
title: "The Pathology of Privacy"
subtitle: "What's the state of privacy today? Protecting privacy is a collective effort. It follows that consent is over-rated and ignorance is rational."
author:
  - name: Johannes Himmelreich
    id: jh
    orcid: 0000-0002-2163-0082
    email: jrhimmel@syr.edu
    affiliation: 
      - name: Maxwell School at Syracuse University
        city: Syracuse
        state: NY
        url: www.syracuse.edu
date: "2024-02-26"
date-modified: "2024-07-12"
categories: ["Foundational Concept", "Privacy", "Data Collection"]
toc: true
draft: false
code-link: true
code-copy: true
title-block-banner: true
comments: false
image: images/privacy.png #path/to/image.png
include-in-header: meta.html
filters:
   - lightbox
lightbox: 
  match: auto
  effect: fade
  desc-position: left
  css-class: "lightwidth"
---

::: callout-note
## Main idea

Protecting privacy is a collective effort. Individuals can only do so much to protect personal information. This undermines the importance of consent and makes it rational to not know the many ways in which your privacy is violated.
:::

# Introduction

The Parks & Recreation TV series has this character of Ron Swanson who --- other than being very fond of breakfast foods --- is a freedom-loving individualist with a deep disdain for rules or any dependence on others. He also takes privacy *very* seriously.

{{< video https://www.youtube.com/embed/8xn1rO1oQmk >}}

Ron's reaction in this clip is what is surprising and unusual (did I mention he takes privacy *very* seriously). Whereas his *reaction* seems like an exaggeration, the *situation* to which Ron reacts should be familiar to all of us. Especially if you live in a jurisdiction with relatively weak privacy protections, such as the United States, you might wonder once in a while whether, like Ron Swanson, it might be better if you just threw all your devices in the trash. 

But what is it exactly that we'd protect by throwing you phone and you computer away --- what is privacy? And why is it valuable? This post won't answer all these questions (I'll do that in a separate post). Instead, with this post I want us to get clearer on why privacy is so under threat today. I also want to chip away at some conventional wisdom, namely, the idea that consent is the way to protect privacy. I argue that problems for privacy today are such that consent is not a viable solution. 

::: aside 
I'm talking about privacy from the perspective of *ethics* not law and I concentrate on *informational* privacy. By contrast, in the United States, privacy is a prominent legal concept and a foundation of liberalism. 
:::

In short, in this post I discuss why privacy is threatened today and how we should think about defending it. I hope this discussion helps to improve your conversations and discussions about privacy.

# The Pathology of Privacy

I'd bet you probably felt like Ron Swanson yourself some time. Go find out how much companies know about you and how little privacy you have. For example, here is a slide from a presentation by LexisNexis, a data company, advertising the range of source from which they aggregate data for identity verification.

::: column-page-inset-right
![Data used for identity verification by LexisNexis (in 2020)](images/lexis-nexis-network.png){fig-alt="Screenshot of a presentation slide. Logos of various companies — including Visa, Netflix, eBay, BestBuy, and Microsoft — are arranged around a mesh sphere."}
:::

In the United States it is relatively easy to get information on individuals' location, credit score, address, and phone number. To everyone except the cynic --- who is never impressed by bad news -- Ron's shock that "it learns information about me" should be familiar.

Whereas the Ron's shock might be familiar, his reaction is not. We wouldn't throw away our devices to protect privacy at all costs. Practically, you often don't can't protect your information. 

Relatively well-known is the **paradox of privacy**: Despite a desire for privacy, many people share private information freely. What we do isn't aligned with how we talk about or value privacy. Behavior is not aligned with stated commitments and preferences for privacy.

Another less discussed fact is the **pathology of privacy**: You don't really know how your information is shared and there isn't much you can do anyway. 

Whereas the paradox of privacy centers on individuals and their choices, the pathology of privacy focuses on the legal structure and typical psychological dispositions. I'd argue the pathology of privacy gets it right: Protecting information is a structural or collective problem.

Whereas the paradox of privacy insinuates that individuals are inconsistent or irrational, the pathology of privacy suggests the opposite: it's reasonable to *not* worry about privacy. Giving up privacy is a precondition for taking part in social life and technological progress. It is rational to give up privacy in exchange for enjoying progress and human connection while hoping that the structural conditions of privacy improve.

## What do you know?

Sometimes, however, you do know how limited your privacy is. You remember there were privacy settings in that app that you just couldn't bother to check. And you know that online advertisers track you. And you know that you signed something at your doctor's office about sharing your patient data.

But often, privacy today is violated unwittingly. For one, it's basically impossible to be aware of all the ways in which your data is collected, analyzed, and shared. Similarly, nobody can possibly read all the terms and conditions and privacy policies that they are agreeing to.

In short, there is an **information problem**: Individuals don't know — and, to some extent, cannot know — all they need to know to safeguard their privacy. And, arguably, we shouldn't spend much of our time and mental energy on protecting our privacy.

## What can you do?

The second part of the state of individual privacy today has to do with choices. We cannot throw away all of our devices, like Ron Swanson does. The options that we have to protect our privacy, even when we are aware of them, is practically limited. Metadata collection that is required by law is hard to opt out of. Even tech-savvy individuals are outgunned by the professional data, surveillance, and security industry.

And then, we also have a cognitive system that is easy to exploit. As such, websites and apps use various forms of deception to get you to do things.[^1] You end up subscribing to a service without knowing, agreeing to terms without having read them, or clicking "Continue" because the other button, "No," was made hard to see.

[^1]: See <https://www.deceptive.design>

::: column-margin
![Deceptive design](images/alexapattern-01.jpeg){width="337"}
:::

Finally, much information is not unique to individuals. This means that protecting that information is not up to you. How much you can do to protect your privacy has in-principle limits when so many others have --- and can share --- the same information that you consider personal to you. Because much information is shared across a large number of people, no individual can control how their information is shared. This is sometimes called the **externality of data**.

One clear example of this is genetic information. Because most genetic information is shared and not unique to one individual, you can't protect most of your genetic information. You can't stop you aunt or brother from using a genetic testing service. Still, you would consider your genetic information, even parts of it, to be your personal information.

Another example is tastes in music, books, movies, or fashion. Conditional on features such as age, income, gender, race, nationality, and job, peoples' tastes in music, books, or fashion are somewhat similar. This means that even if _you_ don't share with Amazon what music, books, movies, or clothes you like, if someone does, who is enough like you, Amazon can infer this information about you. 

# What follows?

In sum, **privacy is inadvertently infringed on a regular basis** in two respects

1.  Individuals hold **false beliefs** about the state of their privacy and the options in front of them (because of deceptive design, the vastness of data collection, or ridiculously obscure terms and conditions).

2.  Individuals have limited options to safeguard their privacy (because many things you can't opt-out of in practice, and because of the externality of data that much personal information is not unique to individuals)

These two points, to my mind, describe in a nutshell the conditions of privacy --- or lack thereof --- today. What do we take away from this?

## Privacy as a collective property

Privacy can still be protected --- just not by individuals on their own. The protection of privacy is a collective action problem. Collectively, with the right norms or laws and their enforcement, we can protect our privacy. It is only that, as individuals there is only so much that each of us can do.

Together, however, we could make terms and conditions more accessible and informative, we could foster competition and alternatives to services and products where currently we find it hard to opt-out (interoperability can counter the natural monopoly tendency of social networks, for example). 

In fact, such a collective approach to safeguarding privacy is necessary because of the externality of data. Norms are required to restrict how others share information because their doing so affects me --- and might harm me. My great cousin and I own shared information, i.e., our genetic data. Insofar as it is a potential harm to me if this data were passed on to a third party, harm prevention could be _one_ reason to restrict how and with whom my great cousin can share our shared information.  

## Against Consent

We should abandon one idea that is deeply associated with privacy: consent. Typically, we think of consent as a sufficient condition for waiving privacy claims. That is, if someone agrees to share information that is theirs, then it's OK to use their data. For example, if someone consents that the picture they uploaded to an online photo service is used by that service to train a generative AI model, then it is OK for the service to do so, since the customer consented to their photos being used in this way.

Of course, consent only plays this role of making things OK that otherwise wouldn't be OK if this consent is _informed_ consent. And, crucially, in many cases in which we click "I agree" online, our consent is somewhat uninformed because of the pathology of privacy (we can't process and know all the ways in which our data will be used, and we often lack a meaningful alternative to agreeing). As such, consent can only play a very limited role in privacy governance for practical reasons --- because the bar for _meaningful_ or _informed_ consent is so high that, in practice, we can't reach it.

However, the externality of data --- that personal information is often shared between individuals --- is a further reason to abandon consent in privacy governance. Whereas the issue of informed consent is a practical limit to make consent worthwhile (we don't have the time to make up our mind), the externality of data is an in-principle limit on using consent: the consent isn't mine to give since the data isn't only about me. 

# What should we do?

The upshot is that we need to rethink privacy how to protect it. I argued that privacy is not up to individuals and that, insofar as that's true, consent can't be a tool for privacy governance. When we think about when it is OK to share information about people, we need to take a different approach. What could that look like?

The short answer is, we need collective privacy governance in the form of norms (such as laws) and enforcement (not only through the state but also though watchdog organizations).  

The long answer would require another post. Maybe I'll write that in the future. In the meantime: there is really no shortage of proposals on what privacy laws could look like, what kind of enforcement we would need, and why privacy is still worth protecting. The main aim of this post was to move our attention away from consent and individualist approaches to privacy governance to make the case for a collective approach --- for which, luckily, we are not entirely unprepared.

