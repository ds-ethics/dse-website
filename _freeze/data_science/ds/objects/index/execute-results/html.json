{
  "hash": "842c9042da2a510082418f2448f44c02",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: '{{< animate fadeInDown \"Bias in AI: detection and mitigation\"delay=.6s >}}'\nsubtitle: '{{< animate fadeInDown \"Enable users to detect and mitigate bias, using the example of the COMPAS Recidivism Risk Score Data and Analysis Dataset. Users will be equiped with concrete strategies to first detect, and secondly mitigate bias\"delay=.6s >}}'\nauthor:\n  - Jorge Roa\n  - Carlo Gre√ü\n  - Hannah Schweren\ndate: '2023-12-05'\ncategories:\n  - Advanced\n  - Bias in AI\n  - Mitigation\ntoc: true\ndraft: false\ncode-link: true\ncode-copy: true\ntitle-block-banner: true\ncomments: false\nimage: images/dalle.png\ninclude-in-header: meta.html\nformat: html\nfilters:\n  - lightbox\n  - webr\nlightbox:\n  match: auto\n  effect: fade\n  desc-position: left\n  css-class: lightwidth\nwebr:\n  packages:\n    - ggplot2\n    - dplyr\n  show-startup-message: false\n  show-header-message: false\n---\n\n<br>\n\n# Introduction\n\nThis notebook offers a detailed guide that includes both code and explanations aimed at enabling users to identify and counteract bias within data, specifically using the COMPAS Recidivism Risk Score Data and Analysis Dataset as a case study. It provides users with practical strategies to first detect and then mitigate bias, laying a foundational approach for handling biases effectively in algorithmic processes. The tutorial is designed as an introductory step towards fostering an understanding of the biases that can infiltrate algorithms and promoting the development of ethical AI practices. This is particularly critical in contexts where algorithmic decisions intersect with policy-making, potentially influencing societal outcomes. Through this guide, users will not only learn to recognize biases but also implement measures to address these biases, thereby enhancing the fairness and integrity of AI systems in public and private sectors.\n\n<br>\n\n# Overview\n\nThe COMPAS dataset, used by an algorithm predicting recidivism risk, has become a key example in the study of algorithmic bias and fairness. It includes demographic and criminal history data. Analyses revealed racial disparities in risk assessments, with the algorithm tending to overestimate recidivism risk for Black defendants and underestimate it for White defendants.\n\nThis tutorial is divided into three parts:\n\n1.- *Introduction to Bias Detection Metrics*: We will introduce different metrics to detect bias, providing a smooth introduction to the topic and helping users gain a better understanding of the issue.\n\n2.- *Replication of Biased Output with a Feed Forward Neural Network*: In this step, we will replicate the biased output using a Feed Forward Neural Network. This hands-on exercise will provide users with practical experience in generating predictions and raise awarness for the biased output.\n\n3.*-Mitigation of Detected Bias*: The grand finale and most important part of our tutorial! Users will learn effective strategies to mitigate the detected bias. This step is crucial for ethical deep learning, and the tutorial aims to equip users with essential skills dealing with biased results.\n\nBy completing this tutorial, users will acquire valuable skills for future data endeavors. It serves as a foundational step to train users and raise awareness of fairness issues in Deep Learning. \n\n<br>\n\n\n## Background and Prerequisites\n\nThis tutorial is designed for users with a basic understanding of Python and Deep Learning. Users should have a foundational understanding of key concepts in machine learning and neural networks. Familiarity with Python is essential. Additionally, a grasp of linear algebra and calculus will be beneficial for understanding the mathematical underpinnings of deep learning algorithms.\n\n- A solid understanding of model training is crucial, as well as knowledge of common machine learning libraries such as   `Keras` and `scikit-learn`. Users should also be aware of the ethical and policy considerations surrounding machine learning applications, particularly in relation to bias and fairness.\n\n- Lastly, a conceptual understanding of how neural networks operate, including layers, activation functions, and backpropagation, will enhance the learning experience of the user. Overall, a basic background in machine learning fundamentals will help users to engage more effectively with our tutorial.\n\n::: {#c09a8626 .cell message='false' execution_count=1}\n``` {.python .cell-code}\n!pip install pandas numpy matplotlib\n!pip install Aequitas\n!pip install keras_tuner\n!pip install aif360\n!pip install BlackBoxAuditing\n!pip install tensorflow\n```\n:::\n\n\n::: {#48b30feb .cell message='false' execution_count=2}\n``` {.python .cell-code}\n# Data visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"white\", palette=\"muted\", color_codes=True, context=\"talk\")\nfrom IPython import display\n\n# Data manipulation\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\n\n# Aequitas library used to audit models for discrimination and bias\nfrom aequitas.group import Group\nfrom aequitas.bias import Bias\nfrom aequitas.fairness import Fairness\nfrom aequitas.plotting import Plot\nimport matplotlib.pyplot as plt\nimport warnings; warnings.simplefilter('ignore')\n\n# Machine and deep learning libraries\nimport tensorflow as tf\nfrom keras.layers import Input, Dense, Dropout\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nimport keras_tuner as kt\nfrom keras import Input, Model\nfrom keras.layers import Dense, Dropout\nfrom keras.optimizers import Adam\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n\n# AI fairness library\nfrom aif360.algorithms.preprocessing import DisparateImpactRemover\nfrom aif360.datasets import StandardDataset as Dataset\nfrom aif360.metrics import BinaryLabelDatasetMetric\nfrom aif360.algorithms.postprocessing.reject_option_classification import RejectOptionClassification\nfrom aif360.algorithms.preprocessing.reweighing import Reweighing\nfrom collections import OrderedDict\nfrom aif360.metrics import ClassificationMetric\n```\n:::\n\n\n# Data Description\n\nIn this tutorial we are working with the COMPAS Recidivism Risk Score Data and Analysis (Source: Pro Publica, https://www.propublica.org/datastore/dataset/compas-recidivism-risk-score-data-and-analysis) This dataset is a classical example for bias in machine learning. We specifically liked using this dataset as an example because it reveils the possible harmfull negative impact on real world decisions, that algorithms can have and the resulting policy responsibility.\n\nThe tabular dataset is used in U.S. court proceedings to evaluate the probability of a defendant reoffending. It is available in csv format for free and contains the following information (Source: https://mlr3fairness.mlr-org.com/reference/compas.html#pre-processing) :\n\n-  (integer) **age** : The age of defendants.\n\n-  (factor) **c_charge_degree** : The charge degree of defendants. F: Felony M: Misdemeanor\n\n- (factor) **race**: The race of defendants.\n\n- (factor) **age_cat**: The age category of defendants.\n\n- (factor) **score_text**: The score category of defendants.\n\n- (factor) **sex**: The sex of defendants.\n\n- (integer) **priors_count**: The prior criminal records of defendants.\n\n- (integer) **days_b_screening_arrest**: The count of days between screening date and (original) arrest date. If they are too far apart, that may - indicate an error. If the value is negative, that indicate the screening date happened before the arrest date.\n\n- (integer) **decile_score**: Indicate the risk of recidivism (Min=1, Max=10)\n\n- (integer) **is_recid**: Binary variable indicate whether defendant is rearrested at any time.\n\n- (factor) **two_year_recid**: Binary variable indicate whether defendant is rearrested at within two years.\n\n- (numeric) **length_of_stay**: The count of days stay in jail.\n\nIn the course of the tutorial, we'll also work with a version of the COMPAS data, that was processed to work well with the aequitas package - this version of the dataset can be found in this Github repository: https://github.com/dssg/aequitas/tree/master/examples/data. Here, only a subset of the variables is considered, but it includes all important variables for demonstrating the package's benefits. It includes:\n\n- (integer) **entity_id**: ID variable\n\n- (integer) **score**: Risk score of defendants, binary\n\n- (factor) **label_value**: Binary variable indicate whether defendant is rearrested\n\n- (factor) *race*: The race of defendants.\n\n- (factor) **sex**: The sex of defendants\n\n- (factor) **age_cat**: The age category of defendants\n\n<br>\n\n# Part 1: Data Exploration and Bias Detection\n\n**Note that the first part of this tutorial is largely based on the documentation of the aequitas-library (https://dssg.github.io/aequitas/examples/compas_demo.html?highlight=xtab). Since the COMPAS data is a widely-known and commonly used data set for showing issues with biased data, the authors used it for demonstrating the library's core functions. Instead of linking the documentation, we decided to include the most important features of the library in the first part of our tutorial, partially adapting some code. More, we adjusted some codes in order to show the metrics that were most important to us for demonstrating bias in the compas data.**\n\n<br>\n\nFor a first overview of the data, we load it directly from GitHub. Note that we use a version of the Compas data here that is explicitly well-suited for the Aequitas library, with a restricted number of columns and slightly deviating variable names. From printing the first 5 rows, we can retrieve that an ID variable, a (binary) risk score, a (binary) recidivism indicator, and three demographic variables (race, sex, age) are included.\n\n\n\n## 1.1 Data Download\n\n::: {#d0b36c0f .cell execution_count=4}\n``` {.python .cell-code}\n# Load the data\n\ndf_compas_aeq = pd.read_csv(\"https://raw.githubusercontent.com/dssg/aequitas/master/examples/data/compas_for_aequitas.csv\")\ndf_compas_aeq.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=24}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>entity_id</th>\n      <th>score</th>\n      <th>label_value</th>\n      <th>race</th>\n      <th>sex</th>\n      <th>age_cat</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>Other</td>\n      <td>Male</td>\n      <td>Greater than 45</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>African-American</td>\n      <td>Male</td>\n      <td>25 - 45</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>African-American</td>\n      <td>Male</td>\n      <td>Less than 25</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>African-American</td>\n      <td>Male</td>\n      <td>Less than 25</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>6</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>Other</td>\n      <td>Male</td>\n      <td>25 - 45</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n<br>\n\n## Exploratory Data Visualization\n\n### Distribution of Defendants by Demographics (Race, Age, Sex) and Risk Scores\n\n<br>\n\nAs a first step, we are exploring the distribution of our defendant data with regards to demographic characteristics and the calculated risk scores. As we can see, African-Americans, Caucasians, males, and defendants aged 25-45 are the subgroups that are highly represented in the data. Additionally, we can already see from the plots that African-Americans and defendants aged under 25 are the only subgroups where the majority has been assigned a high risk score.\n\n::: {#4f3e2ade .cell execution_count=5}\n``` {.python .cell-code code-fold=\"true\"}\nReds_palette = sns.diverging_palette(204, 0, n=2)\n\n# Create a figure with 3 subplots (3 rows, 1 column)\nfig, axes = plt.subplots(3, 1, figsize=(8, 16))\n\n# race\nby_race = sns.countplot(\n    ax=axes[0],\n    x=\"race\",\n    hue=\"score\",\n    data=df_compas_aeq,\n    palette=Reds_palette\n)\n\naxes[0].set_title(\"Distribution of Defendants by Race and Risk Score (Decile)\")\naxes[0].set_xlabel(\"Race\")\naxes[0].set_ylabel(\"Count\")\naxes[0].legend(loc='upper right', title='Risk Score Decile')\naxes[0].grid(True, linestyle='--', linewidth=0.5)\naxes[0].tick_params(axis='x', rotation=45)\n\n# sex\nby_sex = sns.countplot(\n    ax=axes[1],\n    x=\"sex\",\n    hue=\"score\",\n    data=df_compas_aeq,\n    palette=Reds_palette\n)\n\n# Add title and labels\naxes[1].set_title(\"Distribution of Defendants by Sex and Risk Score (Decile)\")\naxes[1].set_xlabel(\"Sex\")\naxes[1].set_ylabel(\"Count\")\n\n# sex\naxes[1].legend(loc='upper right', title='Risk Score')\naxes[1].grid(True, linestyle='--', linewidth=0.5)\naxes[1].tick_params(axis='x', rotation=45)\n\n# Create countplot for age\nby_age = sns.countplot(\n    ax=axes[2],\n    x=\"age_cat\",\n    hue=\"score\",\n    data=df_compas_aeq,\n    palette=Reds_palette\n)\n\naxes[2].set_title(\"Distribution of Defendants by Age and Risk Score (Decile)\")\naxes[2].set_xlabel(\"Age Category\")\naxes[2].set_ylabel(\"Count\")\n\naxes[2].legend(loc='upper right', title='Risk Score')\n\naxes[2].grid(True, linestyle='--', linewidth=0.5)\n\naxes[2].tick_params(axis='x', rotation=45)\n\nplt.tight_layout()\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-1.png){width=739 height=1504}\n:::\n:::\n\n\n<br>\n\n## Distribution of Defendants by Demographics and Recidivism\n\nNext, we are looking at the same demographic subgroups and whether the defendants actually committed crime again. We can already see, that there seems to be a mismatch between the assigned risk scores and the recidivism patterns.\n\n::: {#39e8fbc5 .cell execution_count=6}\n``` {.python .cell-code code-fold=\"true\"}\ncoolwarm_two_colors = sns.color_palette(\"coolwarm\", n_colors=2)\ncoolwarm_palette = sns.color_palette(\"coolwarm\", as_cmap=True)\n\n\n# Create a figure with 3 subplots (3 rows, 1 column)\nfig, axes = plt.subplots(3, 1, figsize=(8, 16))\n\n# Create countplot for race\nlabel_by_race = sns.countplot(\n    ax=axes[0],\n    x=\"race\",\n    hue=\"label_value\",\n    data=df_compas_aeq,\n    palette=coolwarm_two_colors\n)\n\n# Add title and labels for race\naxes[0].set_title(\"Levels of recidivism by Race\")\naxes[0].set_xlabel(\"Race\")\naxes[0].set_ylabel(\"Count\")\naxes[0].grid(True, linestyle='--', linewidth=0.5)\naxes[0].legend(loc='upper right', title='Recidivism')\naxes[0].tick_params(axis='x', rotation=45)\n\n# Create countplot for sex\nlabel_by_sex = sns.countplot(\n    ax=axes[1],\n    x=\"sex\",\n    hue=\"label_value\",\n    data=df_compas_aeq,\n    palette=coolwarm_two_colors\n)\n\n# Add title and labels for sex\naxes[1].set_title(\"Levels of recidivism by Sex\")\naxes[1].set_xlabel(\"Sex\")\naxes[1].set_ylabel(\"Count\")\naxes[1].grid(True, linestyle='--', linewidth=0.5)\naxes[1].legend(loc='upper right', title='Recidivism')\naxes[1].tick_params(axis='x', rotation=45)\n\n# Create countplot for age category\nlabel_by_age = sns.countplot(\n    ax=axes[2],\n    x=\"age_cat\",\n    hue=\"label_value\",\n    data=df_compas_aeq,\n    palette=coolwarm_two_colors\n)\n\n# Add title and labels for age category\naxes[2].set_title(\"Levels of recidivism by Age Category\")\naxes[2].set_xlabel(\"Age Category\")\naxes[2].set_ylabel(\"Count\")\naxes[2].grid(True, linestyle='--', linewidth=0.5)\naxes[2].legend(loc='upper right', title='Recidivism')\n\n# Adjust layout\nplt.tight_layout()\n\n# Display the plot\nplt.show()\n\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-1.png){width=735 height=1503}\n:::\n:::\n\n\n<br>\n\n## Introducing the Aequitas-Library\n\nAfter eyeballing our data set and noticing that there might be some fairness issues, we can now use the Aequitas library to calculate common metrics that indicate biases in subgroups. More specifically, we are using the library's Group() class that evaluates biases across all demographic subgroups in the dataset. Note here that the library requires the input data to have columns named \"score\" and \"label_value\". These columns are by default used to calculate the bias metrics.\n\nIn order to use Aequitas for your purposes, you should rename the columns that you want to check for potential biases to \"score\" and \"label_value\". Additionally, at least one column needs to include grouping information (in our example, several demographic variables). ID variables as entity_id are by default not treated as grouping variables.\n\nThe following code chunk calculates these metrices for all demographic subgroups using the get_crosstabs function, based on the risk score and the label_value, which indicates the recidivism.\n\n::: {#d8e7bc91 .cell execution_count=7}\n``` {.python .cell-code code-fold=\"true\"}\ng = Group()\nxtab, _ = g.get_crosstabs(df_compas_aeq)\nxtab\n```\n\n::: {.cell-output .cell-output-display execution_count=27}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model_id</th>\n      <th>score_threshold</th>\n      <th>k</th>\n      <th>attribute_name</th>\n      <th>attribute_value</th>\n      <th>accuracy</th>\n      <th>tpr</th>\n      <th>tnr</th>\n      <th>for</th>\n      <th>fdr</th>\n      <th>...</th>\n      <th>pprev</th>\n      <th>fp</th>\n      <th>fn</th>\n      <th>tn</th>\n      <th>tp</th>\n      <th>group_label_pos</th>\n      <th>group_label_neg</th>\n      <th>group_size</th>\n      <th>total_entities</th>\n      <th>prev</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>binary 0/1</td>\n      <td>3317</td>\n      <td>race</td>\n      <td>African-American</td>\n      <td>0.638258</td>\n      <td>0.720147</td>\n      <td>0.551532</td>\n      <td>0.349540</td>\n      <td>0.370285</td>\n      <td>...</td>\n      <td>0.588203</td>\n      <td>805</td>\n      <td>532</td>\n      <td>990</td>\n      <td>1369</td>\n      <td>1901</td>\n      <td>1795</td>\n      <td>3696</td>\n      <td>7214</td>\n      <td>0.514340</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>binary 0/1</td>\n      <td>3317</td>\n      <td>race</td>\n      <td>Asian</td>\n      <td>0.843750</td>\n      <td>0.666667</td>\n      <td>0.913043</td>\n      <td>0.125000</td>\n      <td>0.250000</td>\n      <td>...</td>\n      <td>0.250000</td>\n      <td>2</td>\n      <td>3</td>\n      <td>21</td>\n      <td>6</td>\n      <td>9</td>\n      <td>23</td>\n      <td>32</td>\n      <td>7214</td>\n      <td>0.281250</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>binary 0/1</td>\n      <td>3317</td>\n      <td>race</td>\n      <td>Caucasian</td>\n      <td>0.669927</td>\n      <td>0.522774</td>\n      <td>0.765457</td>\n      <td>0.288125</td>\n      <td>0.408665</td>\n      <td>...</td>\n      <td>0.348003</td>\n      <td>349</td>\n      <td>461</td>\n      <td>1139</td>\n      <td>505</td>\n      <td>966</td>\n      <td>1488</td>\n      <td>2454</td>\n      <td>7214</td>\n      <td>0.393643</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>binary 0/1</td>\n      <td>3317</td>\n      <td>race</td>\n      <td>Hispanic</td>\n      <td>0.660911</td>\n      <td>0.443966</td>\n      <td>0.785185</td>\n      <td>0.288591</td>\n      <td>0.457895</td>\n      <td>...</td>\n      <td>0.298273</td>\n      <td>87</td>\n      <td>129</td>\n      <td>318</td>\n      <td>103</td>\n      <td>232</td>\n      <td>405</td>\n      <td>637</td>\n      <td>7214</td>\n      <td>0.364207</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>binary 0/1</td>\n      <td>3317</td>\n      <td>race</td>\n      <td>Native American</td>\n      <td>0.777778</td>\n      <td>0.900000</td>\n      <td>0.625000</td>\n      <td>0.166667</td>\n      <td>0.250000</td>\n      <td>...</td>\n      <td>0.666667</td>\n      <td>3</td>\n      <td>1</td>\n      <td>5</td>\n      <td>9</td>\n      <td>10</td>\n      <td>8</td>\n      <td>18</td>\n      <td>7214</td>\n      <td>0.555556</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0</td>\n      <td>binary 0/1</td>\n      <td>3317</td>\n      <td>race</td>\n      <td>Other</td>\n      <td>0.665782</td>\n      <td>0.323308</td>\n      <td>0.852459</td>\n      <td>0.302013</td>\n      <td>0.455696</td>\n      <td>...</td>\n      <td>0.209549</td>\n      <td>36</td>\n      <td>90</td>\n      <td>208</td>\n      <td>43</td>\n      <td>133</td>\n      <td>244</td>\n      <td>377</td>\n      <td>7214</td>\n      <td>0.352785</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0</td>\n      <td>binary 0/1</td>\n      <td>3317</td>\n      <td>sex</td>\n      <td>Female</td>\n      <td>0.653763</td>\n      <td>0.608434</td>\n      <td>0.678930</td>\n      <td>0.242537</td>\n      <td>0.487310</td>\n      <td>...</td>\n      <td>0.423656</td>\n      <td>288</td>\n      <td>195</td>\n      <td>609</td>\n      <td>303</td>\n      <td>498</td>\n      <td>897</td>\n      <td>1395</td>\n      <td>7214</td>\n      <td>0.356989</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0</td>\n      <td>binary 0/1</td>\n      <td>3317</td>\n      <td>sex</td>\n      <td>Male</td>\n      <td>0.653721</td>\n      <td>0.629132</td>\n      <td>0.675799</td>\n      <td>0.330100</td>\n      <td>0.364637</td>\n      <td>...</td>\n      <td>0.468465</td>\n      <td>994</td>\n      <td>1021</td>\n      <td>2072</td>\n      <td>1732</td>\n      <td>2753</td>\n      <td>3066</td>\n      <td>5819</td>\n      <td>7214</td>\n      <td>0.473105</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0</td>\n      <td>binary 0/1</td>\n      <td>3317</td>\n      <td>age_cat</td>\n      <td>25 - 45</td>\n      <td>0.647846</td>\n      <td>0.626257</td>\n      <td>0.666216</td>\n      <td>0.323112</td>\n      <td>0.385135</td>\n      <td>...</td>\n      <td>0.468240</td>\n      <td>741</td>\n      <td>706</td>\n      <td>1479</td>\n      <td>1183</td>\n      <td>1889</td>\n      <td>2220</td>\n      <td>4109</td>\n      <td>7214</td>\n      <td>0.459723</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0</td>\n      <td>binary 0/1</td>\n      <td>3317</td>\n      <td>age_cat</td>\n      <td>Greater than 45</td>\n      <td>0.704315</td>\n      <td>0.427711</td>\n      <td>0.832096</td>\n      <td>0.241117</td>\n      <td>0.459391</td>\n      <td>...</td>\n      <td>0.250000</td>\n      <td>181</td>\n      <td>285</td>\n      <td>897</td>\n      <td>213</td>\n      <td>498</td>\n      <td>1078</td>\n      <td>1576</td>\n      <td>7214</td>\n      <td>0.315990</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0</td>\n      <td>binary 0/1</td>\n      <td>3317</td>\n      <td>age_cat</td>\n      <td>Less than 25</td>\n      <td>0.617397</td>\n      <td>0.739583</td>\n      <td>0.458647</td>\n      <td>0.424528</td>\n      <td>0.360360</td>\n      <td>...</td>\n      <td>0.653368</td>\n      <td>360</td>\n      <td>225</td>\n      <td>305</td>\n      <td>639</td>\n      <td>864</td>\n      <td>665</td>\n      <td>1529</td>\n      <td>7214</td>\n      <td>0.565075</td>\n    </tr>\n  </tbody>\n</table>\n<p>11 rows √ó 27 columns</p>\n</div>\n```\n:::\n:::\n\n\n<br>\n\nAdditionally, we can use list_absolute_metrics() for an improved overview grouped by the demographics and with rounded values for the metrics.\n\n<br>\n\n::: {#981395f0 .cell execution_count=8}\n``` {.python .cell-code code-fold=\"true\"}\nabsolute_metrics = g.list_absolute_metrics(xtab)\nxtab[['attribute_name', 'attribute_value'] + absolute_metrics].round(2)\n```\n\n::: {.cell-output .cell-output-display execution_count=28}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>attribute_name</th>\n      <th>attribute_value</th>\n      <th>accuracy</th>\n      <th>tpr</th>\n      <th>tnr</th>\n      <th>for</th>\n      <th>fdr</th>\n      <th>fpr</th>\n      <th>fnr</th>\n      <th>npv</th>\n      <th>precision</th>\n      <th>ppr</th>\n      <th>pprev</th>\n      <th>prev</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>race</td>\n      <td>African-American</td>\n      <td>0.64</td>\n      <td>0.72</td>\n      <td>0.55</td>\n      <td>0.35</td>\n      <td>0.37</td>\n      <td>0.45</td>\n      <td>0.28</td>\n      <td>0.65</td>\n      <td>0.63</td>\n      <td>0.66</td>\n      <td>0.59</td>\n      <td>0.51</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>race</td>\n      <td>Asian</td>\n      <td>0.84</td>\n      <td>0.67</td>\n      <td>0.91</td>\n      <td>0.12</td>\n      <td>0.25</td>\n      <td>0.09</td>\n      <td>0.33</td>\n      <td>0.88</td>\n      <td>0.75</td>\n      <td>0.00</td>\n      <td>0.25</td>\n      <td>0.28</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>race</td>\n      <td>Caucasian</td>\n      <td>0.67</td>\n      <td>0.52</td>\n      <td>0.77</td>\n      <td>0.29</td>\n      <td>0.41</td>\n      <td>0.23</td>\n      <td>0.48</td>\n      <td>0.71</td>\n      <td>0.59</td>\n      <td>0.26</td>\n      <td>0.35</td>\n      <td>0.39</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>race</td>\n      <td>Hispanic</td>\n      <td>0.66</td>\n      <td>0.44</td>\n      <td>0.79</td>\n      <td>0.29</td>\n      <td>0.46</td>\n      <td>0.21</td>\n      <td>0.56</td>\n      <td>0.71</td>\n      <td>0.54</td>\n      <td>0.06</td>\n      <td>0.30</td>\n      <td>0.36</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>race</td>\n      <td>Native American</td>\n      <td>0.78</td>\n      <td>0.90</td>\n      <td>0.62</td>\n      <td>0.17</td>\n      <td>0.25</td>\n      <td>0.38</td>\n      <td>0.10</td>\n      <td>0.83</td>\n      <td>0.75</td>\n      <td>0.00</td>\n      <td>0.67</td>\n      <td>0.56</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>race</td>\n      <td>Other</td>\n      <td>0.67</td>\n      <td>0.32</td>\n      <td>0.85</td>\n      <td>0.30</td>\n      <td>0.46</td>\n      <td>0.15</td>\n      <td>0.68</td>\n      <td>0.70</td>\n      <td>0.54</td>\n      <td>0.02</td>\n      <td>0.21</td>\n      <td>0.35</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>sex</td>\n      <td>Female</td>\n      <td>0.65</td>\n      <td>0.61</td>\n      <td>0.68</td>\n      <td>0.24</td>\n      <td>0.49</td>\n      <td>0.32</td>\n      <td>0.39</td>\n      <td>0.76</td>\n      <td>0.51</td>\n      <td>0.18</td>\n      <td>0.42</td>\n      <td>0.36</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>sex</td>\n      <td>Male</td>\n      <td>0.65</td>\n      <td>0.63</td>\n      <td>0.68</td>\n      <td>0.33</td>\n      <td>0.36</td>\n      <td>0.32</td>\n      <td>0.37</td>\n      <td>0.67</td>\n      <td>0.64</td>\n      <td>0.82</td>\n      <td>0.47</td>\n      <td>0.47</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>age_cat</td>\n      <td>25 - 45</td>\n      <td>0.65</td>\n      <td>0.63</td>\n      <td>0.67</td>\n      <td>0.32</td>\n      <td>0.39</td>\n      <td>0.33</td>\n      <td>0.37</td>\n      <td>0.68</td>\n      <td>0.61</td>\n      <td>0.58</td>\n      <td>0.47</td>\n      <td>0.46</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>age_cat</td>\n      <td>Greater than 45</td>\n      <td>0.70</td>\n      <td>0.43</td>\n      <td>0.83</td>\n      <td>0.24</td>\n      <td>0.46</td>\n      <td>0.17</td>\n      <td>0.57</td>\n      <td>0.76</td>\n      <td>0.54</td>\n      <td>0.12</td>\n      <td>0.25</td>\n      <td>0.32</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>age_cat</td>\n      <td>Less than 25</td>\n      <td>0.62</td>\n      <td>0.74</td>\n      <td>0.46</td>\n      <td>0.42</td>\n      <td>0.36</td>\n      <td>0.54</td>\n      <td>0.26</td>\n      <td>0.58</td>\n      <td>0.64</td>\n      <td>0.30</td>\n      <td>0.65</td>\n      <td>0.57</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n<br>\n\nNext, we can use the information on the metrics that have been calculated by the previous chunk to plot the present biases. For that purpose, the `Plot()` class is used and stored in a variable. Afterwards, this variable can be used to plot the metrics of interest. The next code chunk exemplarily plots the **false positive rate** for all subgroups. In the context of our data, false positive cases are present when defendants are classified high risk although they did not recidivate. As we can see from the plot below, these cases are especially present amoung younger as well as among African- and Native Americans.\n\nAdditionally, the colors by default indicate how many respondents are included in the respective subgroup. The exact number can also be retrieved from the bar labels. Referring to the group sizes, you can see that the two races with they highest FPR are of significantly different size: While there are only 18 Native Americans included in our data, a total of nearly 3,700 African-American defendants are present.  \n\n::: {#44c7b4d0 .cell execution_count=9}\n``` {.python .cell-code code-fold=\"true\"}\naqp = Plot()\nfpr = aqp.plot_group_metric(xtab, 'fpr')\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-10-output-1.png){width=862 height=481}\n:::\n:::\n\n\n<br>\n\nFor better readability, and when only interested in the rates rather then the absolute numbers, we can switch the axes and rotate the x-axis labels:\n\n<br>\n\n::: {#97700aa1 .cell execution_count=10}\n``` {.python .cell-code code-fold=\"true\"}\nxtab_df = xtab[['attribute_name', 'attribute_value'] + absolute_metrics].round(2).set_index(['attribute_name', 'attribute_value'])\nxtab_df = xtab_df.reset_index()\n\n# Create a figure for the plot\nplt.figure(figsize=(9, 6))\n\n# Create a bar plot for FPR\nax = sns.barplot(x='attribute_value', y='fpr', hue='attribute_name', data=xtab_df, palette='coolwarm', dodge=True)\nax.set_xticklabels(ax.get_xticklabels(), rotation=60, ha='right')\n\n\n# Add title and labels\nplt.title('False Positive Rate (FPR) by Attribute')\nplt.xlabel('Attribute Value')\nplt.ylabel('FPR')\n\n# Rotate x-axis labels for better readability\nplt.xticks(rotation=60)\n\n# Show the plot\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-11-output-1.png){width=764 height=689}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n# References\n\n------------------------------------------------------------------------\n\n\n::: {.callout-tip}\n## References\n\n- Regean, Mary. 2021. \"Understanding bias and fairness in AI system\". [URL](https://towardsdatascience.com/understanding-bias-and-fairness-in-aisystems-6f7fbfe267f3)\n\n- Anaconda, 2021 State of Data Science Report. [URL](https://know.anaconda.com/rs/387-XNW-688/images/Anaconda-2021-SODS-Report-Final.pdf)\n\n- Clark, Andrew. September 19, 2022. \"Top bias metrics and how they work\". Monitaur. [URL](https://www.monitaur.ai/blog-posts/top-bias-metricsand-how-they-work)\n\n- Feldman et al. ‚ÄûCertifying and Removing Disparate Impact‚Äú. Proceedings of the 21th ACM SIGKDD International Conference on KnowledgeDiscovery and Data Mining, ACM, 2015, S. 259‚Äì68. DOI.org (Crossref), [URL](https://doi.org/10.1145/2783258.2783311).\n\n- Towards datascience, \"AI Fairness ‚Äî Explanation of Disparate Impact Remover\". [URL](https://towardsdatascience.com/ai-fairness-explanation-of-disparate-impact-remover-ce0da59451f1)\n\n- Lee, Nicol Turner, Paul Resnick, and Genie Barton. \"Algorithmic bias detection and mitigation: Best practices and policies to reduce consumer harms.\" Brookings Institute: Washington, DC, USA 2 (2019)\n:::\n\n::: callout-note\nCite this page: Roa, J. (2023, April 16). *Understanding R Objects: Data Structures and Classes in R*. /. [URL](https://www.hertiecodingclub.com/learn/rstudio/rstudio101/)\n:::\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}