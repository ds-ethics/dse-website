{
  "hash": "a5887fa3e87b4191e307423e978c207a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle:  'Gathering Wikipedia Pageviews Data'\nsubtitle: 'Basics of API requests and data analysis in R'\nauthor: [\"Simon Munzert\"]\ndate: \"2024-07-29\"\ncategories: [\"Beginner\", \"APIs\", \"Data collection\"]\ntoc: true\ndraft: false\ncode-link: true\ncode-copy: true\ntitle-block-banner: true\ncomments: false\nimage: ../images/wikipedia-logo.png\ninclude-in-header: meta.html\n---\n\n\n\n# Introduction \n\n------------------------------------------------------------------------\n\nWikipedia contains a wide variety of data, including article contents and metadata such as pageviews, clickstreams, links, backlinks, edits, and revision histories. Pageviews represent the total number of clicks for a specific Wikipedia article. Data on pageviews can be accessed via different channels. First, an [interactive tool](https://pageviews.wmcloud.org/) offers summary data, enabling users to compare the popularity of various search items over a specified period. For data scientists, it is more useful to be able to access data in an automated fashion and integration the data collection process into the coding workflow. To that end, a second access option is more attractive: [Wikimedia's Analytic API](https://doc.wikimedia.org/generated-data-platform/aqs/analytics-api/).\n\n::: {.callout-note}\n**What is an API?**<br>\nA web API (Application Programming Interface) is a set of rules and protocols that allows different software applications to communicate with each other over the internet. A popular form of a web API for data scientists is one that allows applications to access data stored in a database or other data source over the internet. It provides endpoints through which users can send requests to retrieve certain data. For example, a weather data API lets applications request current weather information and forecasts from a weather service's database.\n:::\n\n\n# Gathering Wikipedia Pageviews to measure public attention\n\n------------------------------------------------------------------------\n\n\n::: {.cell}\n\n:::\n\n\nThe code chunks below demonstrates how to collect and graphically display pageviews data using the Wikimedia Analytic API. Usually, in order to access data from an API, the user has to engage with the [documentation](https://doc.wikimedia.org/generated-data-platform/aqs/analytics-api/) to figure out how the endpoints work, under which condition and how the data can be used, etc. In our case, interaction with the API using the R software has been made straightforward with a dedicated R client. That client is provided via the [pageviews package](https://cran.r-project.org/web//packages/pageviews/index.html). \n\nWith the command `article_pageviews()`from that package, we can now gather pageviews of any article on specific Wikipedia projects. Let's use it to settle once and for all who's the most popular classic philosopher! We will restrict ourselves to the English Wikipedia project (`en.wikipedia`) and to three philosopher superstars, namely Socrates, Plato, and Aristotle. e also specify the `argument user_type = \"user\"`, which ensures that we exclude pageviews generated by bots and spiders. Finally, start and end define the period on which we want to collect pageviews data: January 2020 to June 2024.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# get pageviews\nphilosophers_views <-\n  article_pageviews(\n    project = \"en.wikipedia\",\n    article = c(\"Socrates\", \"Plato\", \"Aristotle\"),\n    user_type = \"user\",\n    start = \"2020010100\",\n    end = \"2024063000\"\n  )\n\n# overview\nhead(philosophers_views)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    project language  article     access agent granularity       date views\n1 wikipedia       en Socrates all-access  user       daily 2020-01-01  3527\n2 wikipedia       en Socrates all-access  user       daily 2020-01-02  4037\n3 wikipedia       en Socrates all-access  user       daily 2020-01-03  3891\n4 wikipedia       en Socrates all-access  user       daily 2020-01-04  4073\n5 wikipedia       en Socrates all-access  user       daily 2020-01-05  3740\n6 wikipedia       en Socrates all-access  user       daily 2020-01-06  4266\n```\n\n\n:::\n:::\n\n\nNow, let's get a brief overview of the relative popularity of our philosophers taking the average pageviews over the entire period as benchmark.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# average pageviews by philosopher\nphilosophers_views %>%\n  group_by(article) %>%\n  summarise(mean_views = mean(views, na.rm = TRUE), \n            median_views = median(views, na.rm = TRUE)) %>%\n  arrange(desc(mean_views))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 3\n  article   mean_views median_views\n  <chr>          <dbl>        <dbl>\n1 Aristotle      5184.         5096\n2 Plato          5033.         4934\n3 Socrates       4779.         4646\n```\n\n\n:::\n:::\n\n\nAnd the winner is... Aristotle! He is the most popular philosopher among the three (both in terms of mean and median), followed by Plato and Socrates. However, the differences are not large, and all three philosophers are still very popular on Wikipedia, garnering roughly 5,000 views per day on average.\n\nFinally, we can also plot the frequencies of pageviews over time to identify trends in search behavior. As we can see, there are some ups and downs and some notable spikes, but the overall trend is quite stable over time.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot pageviews\nggplot(philosophers_views, aes(x = date, y = views, color = article)) +\n  geom_line() +\n  labs(title = \"Pageviews of Philosophers on English Wikipedia\",\n       x = \"Date\",\n       y = \"Pageviews\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n \n\n# Conclusion\n\n------------------------------------------------------------------------\n\nCollecting and analyzing Wikipedia data is straightforward and completely free. This allows researchers to utilize and examine a vast amount of data, providing valuable insights to human knowledge production and consumption. Social science research is increasingly incorporating these advancements, as seen in recent studies (e.g., [Göbel and Munzert 2018](https://doi.org/10.1177/0894439317703579); [Shi et al. 2019](https://doi.org/10.1038/s41562-019-0541-6)).\n\nHowever, using Wikipedia data comes with limitations and challenges. Since entries can be read and edited by both humans and machines, the accuracy of content and validity of metadata are not guaranteed. Researchers should also consider that Wikipedia data relies heavily on user-driven content creation, editing, and usage. This reliance can lead to systematic selection bias and issues with data equivalence (e.g., articles on historical political figures may receive fewer views and edits than those on active politicians, for reasons unrelated to their actual significance).\n\nThese caveats are not unique to Wikipedia data; they highlight the general necessity of thoroughly scrutinizing and critically assessing the suitability of any data for addressing substantive research questions.\n\n \n\n# References\n\n------------------------------------------------------------------------\n\n::: {.callout-note}\nLearn more: Denis Cohen, Nick Baumann, Simon Munzert (2019, August 26). *Studying Politics on and with Wikipedia*. MZES Methods Bites. [URL](https://www.mzes.uni-mannheim.de/socialsciencedatalab/article/studying-politics-wikipedia)\n:::",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}