[
  {
    "objectID": "for_instructors/instructors.html",
    "href": "for_instructors/instructors.html",
    "title": "{{< fa solid scale-balanced >}} Ethics <span style='font-size: 18px;'>&#x2715;</span> Data Science {{< fa solid code-branch >}}",
    "section": "",
    "text": "Welcome to the “For Instructors” section of “Data Science X Ethics” – a dedicated space where education meets innovation at the crossroads of data science and ethical responsibility. As educators shaping the next generation of data scientists, your role in integrating ethical considerations into the technical curriculum is more crucial than ever. In this era of rapid technological advancement, teaching data science is not just about coding, algorithms, and data analysis; it’s equally about instilling a strong ethical foundation in your students. Click here to access the Instructor’s Guide for “Data Science X Ethics.”"
  },
  {
    "objectID": "for_instructors/instructors.html#for-instructors",
    "href": "for_instructors/instructors.html#for-instructors",
    "title": "{{< fa solid scale-balanced >}} Ethics <span style='font-size: 18px;'>&#x2715;</span> Data Science {{< fa solid code-branch >}}",
    "section": "For instructors",
    "text": "For instructors\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nPrivacy Activity\n\n\nA simulation on privacy and election data analytics\n\n\n\nActivity\n\n\nPrivacy\n\n\nData Collection\n\n\n\n\n\n\nFeb 28, 2024\n\n\nJohannes Himmelreich\n\n\n8 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "for_instructors/guide/ethics/index.html",
    "href": "for_instructors/guide/ethics/index.html",
    "title": "Ethics",
    "section": "",
    "text": "Your important content or announcement goes here."
  },
  {
    "objectID": "for_instructors/guide/ethics/index.html#important-section-title",
    "href": "for_instructors/guide/ethics/index.html#important-section-title",
    "title": "Ethics",
    "section": "",
    "text": "Your important content or announcement goes here."
  },
  {
    "objectID": "for_instructors/guide/ethics/index.html#subsection-title",
    "href": "for_instructors/guide/ethics/index.html#subsection-title",
    "title": "Ethics",
    "section": "Subsection Title",
    "text": "Subsection Title\nDetails about the subsection. This can include code snippets, explanations, or any relevant information.\n\n# R code goes here\n\n# Example\n\nx &lt;- 1:10\n\ny &lt;- x^2\n\nplot(x, y)\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nCite this page: Lehmann, L. (2023, April 18). Introduction to Social Media Scraping. Hertie Coding Club. URL"
  },
  {
    "objectID": "ethics/ethics/example/index.html",
    "href": "ethics/ethics/example/index.html",
    "title": "Your Document Title",
    "section": "",
    "text": "Your important content or announcement goes here."
  },
  {
    "objectID": "ethics/ethics/example/index.html#important-section-title",
    "href": "ethics/ethics/example/index.html#important-section-title",
    "title": "Your Document Title",
    "section": "",
    "text": "Your important content or announcement goes here."
  },
  {
    "objectID": "ethics/ethics/example/index.html#subsection-title",
    "href": "ethics/ethics/example/index.html#subsection-title",
    "title": "Your Document Title",
    "section": "Subsection Title",
    "text": "Subsection Title\nDetails about the subsection. This can include code snippets, explanations, or any relevant information.\n\n# R code goes here\n\n# Example\n\nx &lt;- 1:10\n\ny &lt;- x^2\n\nplot(x, y)\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nCite this page: Lehmann, L. (2023, April 18). Introduction to Social Media Scraping. Hertie Coding Club. URL"
  },
  {
    "objectID": "ethics/ethics/approaches/index.html",
    "href": "ethics/ethics/approaches/index.html",
    "title": "Approaches",
    "section": "",
    "text": "Main idea\n\n\n\nData science ethics is often seen through the lenses of responsibility or bias. We add to this the moral practice approach.\n\n\n\n\n\n\n\n\nTheory warning\n\n\n\nThis post is very theoretical: meta-theoretical. It offers a hypothesis about the space of theories of data science ethics.\n\n\n\nIntroduction\nSuppose you could magically make one of three things happen.\n\nData science will get a code of ethics and all data scientists will have to take a professional oath on this code.\nData science will solve bias. Data scientists check their implicit biases, teams are more diverse, and data is more representative and inclusive.\nData science will champion reasoning. Data scientists recognize ethical dilemmas, explain them, and work with others to solve them.\n\nEach of these are good options. Here, I want to make a case for the third.\nEach of these options stands for a different approach of data science ethics. Each is backed up by a different view of what kinds of ethical challenges data science raises. As such, each of the three options is a set of tools that fits with its respective view of the challenges. I call these three frameworks the responsibility, bias, and practice approach.\nIt’s important to be clear about your approach for the ethics of data science. Depending on what kind of ethical challenges you think data science raises, you need to chose your tools appropriately.\n\n\n\n\n\n\nTip\n\n\n\nThink hard on how you approach the ethics of data science. Depending on your approach, you will practice ethics of data science totally differently.\n\n\nThis module enables you to see the bigger picture in the ethics of data science. And to make your choice of tools more deliberately.\n\n\nResponsibility Approach\nThe first framework to the ethics of data science centers on people. It says that the general problem in the ethics of data science has to to with data scientists themselves. Data scientists lack a sense of responsibility.\nAccordingly, proponents of this approach, such as Cathy O’Neil, argue that data scientists should take a professional oath. Such an oath, taken with the right intention, is a key part of building a sense a professional responsibility. Such oaths are familiar from other professions, such as medicine, engineering, or public administration. In short, some argue for a care gap: a lack of care, empathy, or concern causes irresponsible data science.\nBut care is not the only ingredient of responsibility. A sense of professional responsibility can be fostered in several ways. For starters, data scientists need to know how their work is used. After all, a lack of responsibility might be not only due to a lack of care or compassion, but also due a lack of knowledge. There might be a information gap: High technical complexity, large-scale division of labor, and ideologies cloud the view of what one is contributing to. This information obstacle stands in the way of responsible data science.\nAnd it’s not always easy to know, ahead of time, how a data science project will play out in practice. And even if we could, individuals can often make very little difference. That means: A sense of responsibility, to be effective, needs to be collectively shared and organized — not an easy thing to do! There is thus, finally an organization gap in data science: responsible individuals lack the structures, places, and power to put responsibility into practice together.\nThe good news is: A lot is being done to build such a shared sense of responsibility in data science. It’s hard for data scientists these days not to be confronted with the harms their work may cause. Data science degrees now regularly include education on ethics. And we see emerging legislation to create incentives for data science to care about ethical issues, such as the EU’s AI Act. In this way, a sense of responsibility can be incentivised extrinsically, it must not emerge from the individuals or the profession.\nIn sum, the responsibility approach to the ethics of data science contends that the ethics of data science should be about people and how they organize themselves. The job of ethics, on this approach, is to make data scientists care and make them aware of the consequences of their work. The strategies to achieve this include ethics codes, professional oaths, interdisciplinary collaboration, collaborative governance, and regulation.\n\n\nBias Approach\nA second framework centers on bias—or rather: biases, since bias comes in many different forms. I think it helps to distinguish at least two forms of bias: cognitive and structural.\nThe first important form of bias is cognitive bias, that is, problematic habits of thought, associations, or assumptions. The ethics of data science might be about the cognitive biases, or at least, the biases of its practitioners. As one prominent slogan goes: “All models are biased because humans are biased.”\n\n\nWhat makes some biases problematic and other OK is actually a very tricky question.\nIn addition, an ethics of data science that focuses on bias may also proceed on a structural route. Here the focus is not on biases—implicit or explicit, cognitive or behavioral—of individuals but on the biases reflected in the data. Here the slogan could be that “all models are biased because reality is biased.” That is, data represent the injustices and inequities that are out there in our very imperfect world. By the principle of garbage in and garbage out, data science then encodes these biases, entrenches them, or even makes them worse. Structural bias, i.e., problematic group level distributional differences of certain properties such as income, geography, or interaction with institutions, creeps into data science because these differences are captured by data.\nAgain, there is much to this picture of ethics as bias mitigation. Again, the lessons it imparts were painfully learned. As often with moral progress, society at large and the profession of data science in particular stand in the debt of minority scholars who took on the labor of pointing out what should be obvious; often failing to “get through,” often belittled, discounted, or dismissed.\nData science has a lot to do with bias. But issues of bias are still at most one part of the ethics of data science. The ethics of data science is much broader than bias. For example, constructing a measure of “teacher effectiveness” raises methodological and ethical problems about what it means to be a good teacher, whether collecting the relevant data to measure this violates students’ privacy — issues that have little to do with cognitive or structural biases. Likewise, even paradigmatic examples of bias at work, such as predicting crime or recidivism (where historic injustice is clearly reflected in the data), have to do with more than bias, namely, with the power of the police or the legitimacy of pre-trial detention more broadly.\n\n\n\n\n\n\n\n\nLimits of “bias”\n\n\n\nThe ethics of data science is about much more than bias mitigation.\n\n\nAs the word “reflected” already suggests, biases in data are often symptoms of an underlying problem. They indicate more fundamental problems that cause or sustain biases. The ethics of data science thus should speak to these underlying problems and not only to the bias surfaced in data.\n\n\nMoral Practice Approach\nI am personally drawn to a third approach. The ethics of data science should center on the practice of data science. The idea here is that data science is hard—not just technically but also morally. It involves moral dilemmas or trade-offs that hide behind seemingly technical issues. But the trade-offs are real and it does no good to hide them. The slogan of this third approach would be: “Data Science is a Moral Practice.”\nWhen training a model to predict welfare fraud, data scientists need to balance false positives and false negatives. What should be the “exchange rate” between such errors? Similarly, data scientists regularly face a trade-off between accuracy and fairness. This trade-off present itself, for example, in the question of whether, when wanting to predict recidivism, data on arrests for minor, non-violent offenses—where bias is particularly prominent—should be included.\nThat data science is a moral practice becomes particularly clear when we take data science to be much more than data analysis. Communication plays an important—and often neglected—role in data science. Ethical questions abound in communicating in data science and outside of data analysis more broadly. Are there some projects that a data science team must not take on? How should limits of a product be communicated? How much should be done to protect privacy?\nThe moral practice approach contends that you find such thorny questions at each step in the lifecycle of a data science project. Data scientists face moral dilemmas. That is, they face decisions to which moral considerations are relevant and these considerations pull in opposite directions. Each option has moral reasons in its favor but also against it. The moral practice approach to data science seeks to understand these dilemmas to allow data scientists—or anyone interested—to reason through these dilemmas together, to disagree better, and come to a decision on firmer ground.\nPerhaps the time for the moral practice approach has not yet come. For one, bias and a lack of professional responsibility still pervade data science and all societies at large. As such, the responsibility and the bias approach are urgently needed — they fit the problems that anyone in the profession faces. Moreover, in divided societies, the moral practice approach seems idealistic, naïve, or Panglossian. For it requires that decisions are made rationally, that individuals are willing to reason about their decisions, and that moral reasoning can play a role in deliberations. But is this form of deliberation possible today?\nI think it is. The approach can work under the right conditions. Creating these conditions is up to every team leader. In fact, the greatest threat to moral progress is cynicism. I want to positively reject the cynicism behind the suggestion that “this approach is not going to work anyway.”\nFinally, the moral practice approach is one for the long term. Data science raises old issues—about privacy, justice, and knowledge—anew. These issues are not going away. By contrast, we can hope that the problems that are currently urgent, of problematic biases and a lacking professional responsibility, will be overcome with time.\n\n\nThe Difference an Approach Makes\nHow you approach the ethics of data science is a big decision. To summarize, each approach has its own view on what the problem is that an ethics of data science addresses. Moreover, each approach comes with own tools and strategies that fit the respective problem — and hence with things you need to learn and understand to practice data science ethics. On each of the three approaches, you’d focus on different things.\n\n\n\n\n\n\n\n\n\n\n\nResponsibility\nBias\nMoral Practice\n\n\n\n\nAim\nclose information gap, care gap, and organization gap\nreduce biases (cognitive, structural)\nbetter decision-making processes\n\n\nTools\n\nprofessional oath\nawareness of harms caused by data science projects\nethics codes\n\n\nrepresentative data collection\ntests for construct validity to avoid biased proxy variables\nteam diversity\ntraining and nudging against (implicit) cognitive bias\ndata debiasing methods\n\n\nmoral vocabulary\ncommunication skills\nmoral reasoning\nresearch ethics\n\n\n\n\n\nEven if the ethics of data science may be all about dilemmas, there is no real dilemma here. You can follow all three approaches at once — at least in principle.\nThe point of this article is to allow you to choose your approach more deliberately. This article takes a step back and brings a broader landscape of approaches into view. Moreover, you may need to choose after all. Your time and resources are limited. Even if you could pursue all three approaches in principle, you’ll probably have to choose what you really want to concentrate on in a given situation. I hope that this article clarifies the choice and helps you make the right one."
  },
  {
    "objectID": "data_science/I2DS/i2ds2021/index.html",
    "href": "data_science/I2DS/i2ds2021/index.html",
    "title": "I2DS Tools for Data Science Workshop 2021",
    "section": "",
    "text": "The Introduction to Data Science (IDS) course at the Hertie School in Berlin, Germany, is designed to provide students with a comprehensive understanding of data science principles, techniques, and tools. As part of the course curriculum, students actively engage in organizing workshops and activities to enhance their practical skills and share their knowledge with others.\n \nThese student-led activities, such as the workshop on Tools for Data Science with R, serve as valuable opportunities for participants to gain hands-on experience and learn directly from their peers. The I2DS course encourages students to take the lead in organizing and delivering these workshops, fostering a collaborative learning environment.\n\n\n\n Go to I2DS Tools 2021"
  },
  {
    "objectID": "data_science/data_science.html",
    "href": "data_science/data_science.html",
    "title": "{{< fa solid scale-balanced >}} Ethics <span style='font-size: 18px;'>&#x2715;</span> Data Science {{< fa solid code-branch >}}",
    "section": "",
    "text": "In today’s data-driven world, the power of data science transcends industries, influencing decisions, shaping strategies, and transforming futures. Yet, for many, the field remains shrouded in jargon and complexity. That’s where we step in. Whether you’re a professional looking to leverage data in your field, a student eager to expand your horizons, or simply a data enthusiast, our platform is designed with you in mind.\nThrough a blend of engaging articles, interactive tutorials, and real-world examples, we break down key data science concepts into digestible pieces. From the basics of data analysis and visualization to the intricacies of machine learning and artificial intelligence, we provide a comprehensive learning journey that’s both enjoyable and informative. Our goal is to empower you with the knowledge and skills you need to thrive in the data-driven world."
  },
  {
    "objectID": "data_science/data_science.html#data-science",
    "href": "data_science/data_science.html#data-science",
    "title": "{{< fa solid scale-balanced >}} Ethics <span style='font-size: 18px;'>&#x2715;</span> Data Science {{< fa solid code-branch >}}",
    "section": "Data Science",
    "text": "Data Science\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nYour Document Title\n\n\nSubtitle or Brief Description\n\n\n\nIntermediate\n\n\nCategory2\n\n\nCategory3\n\n\n\n\n\n\nInvalid Date\n\n\nYour Name\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nBias in AI: detection and mitigation\n\n\nEnable users to detect and mitigate bias, using the example of the COMPAS Recidivism Risk Score Data and Analysis Dataset. Users will be equiped with concrete strategies to first detect, and secondly mitigate bias\n\n\n\nAdvanced\n\n\nBias in AI\n\n\nMitigation\n\n\n\n\n\n\nDec 5, 2023\n\n\nJorge Roa, Carlo Greß, Hannah Schweren\n\n\n47 min\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Social Media Scraping\n\n\nLearn the basic tools for scraping Twitter Data in R\n\n\n\nBeginner\n\n\nWebscraping\n\n\nText\n\n\n\n\n\n\nApr 18, 2023\n\n\nLukas Lehmann\n\n\n12 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "data_science/ds/objects/index.html",
    "href": "data_science/ds/objects/index.html",
    "title": "Bias in AI: detection and mitigation",
    "section": "",
    "text": "This notebook offers a detailed guide that includes both code and explanations aimed at enabling users to identify and counteract bias within data, specifically using the COMPAS Recidivism Risk Score Data and Analysis Dataset as a case study. It provides users with practical strategies to first detect and then mitigate bias, laying a foundational approach for handling biases effectively in algorithmic processes. The tutorial is designed as an introductory step towards fostering an understanding of the biases that can infiltrate algorithms and promoting the development of ethical AI practices. This is particularly critical in contexts where algorithmic decisions intersect with policy-making, potentially influencing societal outcomes. Through this guide, users will not only learn to recognize biases but also implement measures to address these biases, thereby enhancing the fairness and integrity of AI systems in public and private sectors."
  },
  {
    "objectID": "data_science/ds/objects/index.html#numeric-values",
    "href": "data_science/ds/objects/index.html#numeric-values",
    "title": "Understanding R Objects: Naming, Data Structures and Classes in R",
    "section": "Numeric values",
    "text": "Numeric values\nA numeric object can be stored as a number.\n\nx &lt;- 6\n\nclass(x)\n\n[1] \"numeric\"\n\nx\n\n[1] 6"
  },
  {
    "objectID": "data_science/ds/objects/index.html#character-values",
    "href": "data_science/ds/objects/index.html#character-values",
    "title": "Understanding R Objects: Naming, Data Structures and Classes in R",
    "section": "Character values",
    "text": "Character values\nStores a sequence of characters, such as letters, numbers, and symbols. A character value is created by enclosing the sequence of characters within quotation marks, either single or double.\n\ncharacter &lt;- \"Welcome to the Hertie Coding Club\"   # creating a character value\n\nCharacter values can be combined using the paste() or paste0() functions, which concatenate two or more character values into a single character value.\n\nx &lt;- \"Welcome to\"\ny &lt;- \"the Hertie Coding Club\"\nz &lt;- paste(x, y, sep = \": \")\n\n\nclass(z)\n\n[1] \"character\"\n\nz\n\n[1] \"Welcome to: the Hertie Coding Club\""
  },
  {
    "objectID": "data_science/ds/objects/index.html#create-a-numeric-vector",
    "href": "data_science/ds/objects/index.html#create-a-numeric-vector",
    "title": "Understanding R Objects: Naming, Data Structures and Classes in R",
    "section": "Create a numeric vector",
    "text": "Create a numeric vector\n\nx &lt;- c(1, 2, 3, 4, 5)\n\nx\n\n[1] 1 2 3 4 5"
  },
  {
    "objectID": "data_science/ds/objects/index.html#access-elements-of-a-vector",
    "href": "data_science/ds/objects/index.html#access-elements-of-a-vector",
    "title": "Understanding R Objects: Naming, Data Structures and Classes in R",
    "section": "Access elements of a vector",
    "text": "Access elements of a vector\nReturns the second element of x, which is 2\n\nx[2] \n\n[1] 2"
  },
  {
    "objectID": "data_science/ds/objects/index.html#perform-arithmetic-operations",
    "href": "data_science/ds/objects/index.html#perform-arithmetic-operations",
    "title": "Understanding R Objects: Naming, Data Structures and Classes in R",
    "section": "Perform arithmetic operations",
    "text": "Perform arithmetic operations\nAdds 2 to each element of x, resulting in the vector c(3, 4, 5, 6, 7)\n\nx + 2 \n\n[1] 3 4 5 6 7"
  },
  {
    "objectID": "data_science/ds/objects/index.html#find-the-length-of-a-vector",
    "href": "data_science/ds/objects/index.html#find-the-length-of-a-vector",
    "title": "Understanding R Objects: Naming, Data Structures and Classes in R",
    "section": "Find the length of a vector:",
    "text": "Find the length of a vector:\n\nlength(x) \n\n[1] 5"
  },
  {
    "objectID": "data_science/ds/objects/index.html#modify-a-vector",
    "href": "data_science/ds/objects/index.html#modify-a-vector",
    "title": "Understanding R Objects: Naming, Data Structures and Classes in R",
    "section": "Modify a vector",
    "text": "Modify a vector\nChanges the third element of x to 10\n\nx[3] &lt;- 10\n\nx\n\n[1]  1  2 10  4  5"
  },
  {
    "objectID": "data_science/ds/objects/index.html#apply-functions-to-a-vector",
    "href": "data_science/ds/objects/index.html#apply-functions-to-a-vector",
    "title": "Understanding R Objects: Naming, Data Structures and Classes in R",
    "section": "Apply functions to a vector",
    "text": "Apply functions to a vector\nReturns the sum of the elements in x, which is 22\n\nsum(x) \n\n[1] 22\n\n\nReturns the mean (average) of the elements in x, which is 4.4\n\nmean(x)\n\n[1] 4.4\n\n\nReturns the maximum value in x, which is 10\n\nmax(x)\n\n[1] 10\n\n\nReturns the minimum value in x, which is 1\n\nmin(x)\n\n[1] 1\n\n\n \nIn addition to numeric values, you can put a wide variety of other data types in a vector in R. Here are some examples:"
  },
  {
    "objectID": "data_science/ds/objects/index.html#character-values-1",
    "href": "data_science/ds/objects/index.html#character-values-1",
    "title": "Understanding R Objects: Naming, Data Structures and Classes in R",
    "section": "Character values",
    "text": "Character values\n\nnames &lt;- c(\"Hertie School\", \"Berlin\", \"Public Policy\")\n\nnames\n\n[1] \"Hertie School\" \"Berlin\"        \"Public Policy\""
  },
  {
    "objectID": "data_science/ds/objects/index.html#logical-values",
    "href": "data_science/ds/objects/index.html#logical-values",
    "title": "Understanding R Objects: Naming, Data Structures and Classes in R",
    "section": "Logical values",
    "text": "Logical values\n\nflags &lt;- c(TRUE, FALSE, TRUE)\n\nflags\n\n[1]  TRUE FALSE  TRUE"
  },
  {
    "objectID": "data_science/ds/objects/index.html#times",
    "href": "data_science/ds/objects/index.html#times",
    "title": "Understanding R Objects: Naming, Data Structures and Classes in R",
    "section": "Times:",
    "text": "Times:\n\ntimes &lt;- as.POSIXct(c(\"2022-01-01 10:00:00\", \"2022-01-01 11:00:00\", \"2022-01-01 12:00:00\"))\n\ntimes\n\n[1] \"2022-01-01 10:00:00 CET\" \"2022-01-01 11:00:00 CET\"\n[3] \"2022-01-01 12:00:00 CET\""
  },
  {
    "objectID": "data_science/ds/objects/index.html#a-list-with-different-type-of-vectors",
    "href": "data_science/ds/objects/index.html#a-list-with-different-type-of-vectors",
    "title": "Understanding R Objects: Naming, Data Structures and Classes in R",
    "section": "A list with different type of vectors",
    "text": "A list with different type of vectors\n\nmy_list &lt;- list(numbers = c(1, 2, 3), names = c(\"Alejandro\", \"Eduardo\", \"Fernanda\"), statement = c(TRUE, FALSE, TRUE))\n\nmy_list\n\n$numbers\n[1] 1 2 3\n\n$names\n[1] \"Alejandro\" \"Eduardo\"   \"Fernanda\" \n\n$statement\n[1]  TRUE FALSE  TRUE"
  },
  {
    "objectID": "data_science/ds/objects/index.html#access-an-element-of-the-list-using-indexing",
    "href": "data_science/ds/objects/index.html#access-an-element-of-the-list-using-indexing",
    "title": "Understanding R Objects: Naming, Data Structures and Classes in R",
    "section": "Access an element of the list using indexing",
    "text": "Access an element of the list using indexing\nWe can access an element of the list using indexing, which is really useful when you are doing functions and storing results or calling them.\nHere this returns the second element of my_list, which is the character vector “names”\n\nmy_list[[2]]\n\n[1] \"Alejandro\" \"Eduardo\"   \"Fernanda\" \n\n\nor also we can access an element of the list using named indexing:\n\nmy_list[[\"names\"]]\n\n[1] \"Alejandro\" \"Eduardo\"   \"Fernanda\""
  },
  {
    "objectID": "data_science/ds/objects/index.html#add-a-new-element-to-the-list",
    "href": "data_science/ds/objects/index.html#add-a-new-element-to-the-list",
    "title": "Understanding R Objects: Naming, Data Structures and Classes in R",
    "section": "Add a new element to the list:",
    "text": "Add a new element to the list:\nAdds a new numeric vector “ages” to my_list\n\nmy_list[[\"ages\"]] &lt;- c(25, 30, 35)\n\nWe can do the same even with a dataframe inside a list\n\ndf &lt;- data.frame(name = c(\"Alejandro\", \"Eduardo\", \"Marifer\"), \n                 age = c(30, 27, 17)) # Create the dataframe\n\nmy_list[[\"my_df\"]] &lt;- df\n\nmy_list\n\n$numbers\n[1] 1 2 3\n\n$names\n[1] \"Alejandro\" \"Eduardo\"   \"Fernanda\" \n\n$statement\n[1]  TRUE FALSE  TRUE\n\n$ages\n[1] 25 30 35\n\n$my_df\n       name age\n1 Alejandro  30\n2   Eduardo  27\n3   Marifer  17"
  },
  {
    "objectID": "data_science/ds/objects/index.html#create-a-nested-list-with-two-elements",
    "href": "data_science/ds/objects/index.html#create-a-nested-list-with-two-elements",
    "title": "Understanding R Objects: Naming, Data Structures and Classes in R",
    "section": "Create a nested list with two elements:",
    "text": "Create a nested list with two elements:\n\nnested_list &lt;- list(numbers = c(1, 2, 3), list_2 = list(letter = \"A\", number = 42))\n\nnested_list\n\n$numbers\n[1] 1 2 3\n\n$list_2\n$list_2$letter\n[1] \"A\"\n\n$list_2$number\n[1] 42\n\n\nFor access a nested element of the list:\n\nnested_list[[\"list_2\"]][[\"number\"]]  \n\n[1] 42\n\n\nAs you can see, we can access the outputs of the list we want. This is useful since you can apply functions to retrieve your data from a nested list or access to a specific output of your model, for example."
  },
  {
    "objectID": "data_science/ds/objects/index.html#createing-a-matrix",
    "href": "data_science/ds/objects/index.html#createing-a-matrix",
    "title": "Understanding R Objects: Naming, Data Structures and Classes in R",
    "section": "Createing a matrix",
    "text": "Createing a matrix\nCreate a matrix with three rows and four columns:\n\nm_example &lt;- matrix(1:12, nrow = 3, ncol = 4)\n\nm_example\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12"
  },
  {
    "objectID": "data_science/ds/objects/index.html#access-an-element-of-the-matrix-using-indexing",
    "href": "data_science/ds/objects/index.html#access-an-element-of-the-matrix-using-indexing",
    "title": "Understanding R Objects: Naming, Data Structures and Classes in R",
    "section": "Access an element of the matrix using indexing",
    "text": "Access an element of the matrix using indexing\nReturns the element in the second row and third column of my_matrix, which is 7\n\nm_example[2, 3] \n\n[1] 8"
  },
  {
    "objectID": "data_science/ds/objects/index.html#modify-an-element-of-the-matrix",
    "href": "data_science/ds/objects/index.html#modify-an-element-of-the-matrix",
    "title": "Understanding R Objects: Naming, Data Structures and Classes in R",
    "section": "Modify an element of the matrix",
    "text": "Modify an element of the matrix\nChanges the element in the first row and fourth column of my_matrix to 20\n\nm_example[1, 4] &lt;- 20  \n\nm_example\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   20\n[2,]    2    5    8   11\n[3,]    3    6    9   12"
  },
  {
    "objectID": "data_science/ds/objects/index.html#create-an-array-with-three-dimensions",
    "href": "data_science/ds/objects/index.html#create-an-array-with-three-dimensions",
    "title": "Understanding R Objects: Naming, Data Structures and Classes in R",
    "section": "Create an array with three dimensions",
    "text": "Create an array with three dimensions\n\narr_example &lt;- array(1:24, dim = c(2, 3, 4))\n\narr_example\n\n, , 1\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\n, , 2\n\n     [,1] [,2] [,3]\n[1,]    7    9   11\n[2,]    8   10   12\n\n, , 3\n\n     [,1] [,2] [,3]\n[1,]   13   15   17\n[2,]   14   16   18\n\n, , 4\n\n     [,1] [,2] [,3]\n[1,]   19   21   23\n[2,]   20   22   24"
  },
  {
    "objectID": "data_science/ds/objects/index.html#access-an-element-of-the-array-using-indexing",
    "href": "data_science/ds/objects/index.html#access-an-element-of-the-array-using-indexing",
    "title": "Understanding R Objects: Naming, Data Structures and Classes in R",
    "section": "Access an element of the array using indexing",
    "text": "Access an element of the array using indexing\nReturns the element in the second row, third column, and fourth layer of my_array, which is 23\n\narr_example[2, 3, 4]\n\n[1] 24"
  },
  {
    "objectID": "data_science/ds/objects/index.html#modify-an-element-of-the-array",
    "href": "data_science/ds/objects/index.html#modify-an-element-of-the-array",
    "title": "Understanding R Objects: Naming, Data Structures and Classes in R",
    "section": "Modify an element of the array",
    "text": "Modify an element of the array\nChanges the element in the second row, second column, and first layer of arr_example to 10\n\narr_example[2, 2, 1] &lt;- 10 \n\narr_example\n\n, , 1\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2   10    6\n\n, , 2\n\n     [,1] [,2] [,3]\n[1,]    7    9   11\n[2,]    8   10   12\n\n, , 3\n\n     [,1] [,2] [,3]\n[1,]   13   15   17\n[2,]   14   16   18\n\n, , 4\n\n     [,1] [,2] [,3]\n[1,]   19   21   23\n[2,]   20   22   24"
  },
  {
    "objectID": "data_science/ds/objects/index.html#create-a-data-frame-multiple-ways",
    "href": "data_science/ds/objects/index.html#create-a-data-frame-multiple-ways",
    "title": "Understanding R Objects: Naming, Data Structures and Classes in R",
    "section": "Create a Data Frame: multiple ways",
    "text": "Create a Data Frame: multiple ways\n1.- Using data.frame() function: This is the most common way to create a data frame in R. You can put as many vectors (variables) as you want.\n\ndf &lt;- data.frame(x = c(1, 2, 3), \n                 y = c(\"a\", \"b\", \"c\"), \n                 z = c(TRUE, FALSE, TRUE))\n\n2.- Using read_csv2() function: You can also create a data frame by importing data from an external file using the read.table() or read.csv() function.\n\ndf &lt;- read.table(\"data.txt\", header = TRUE)\n\n3.- Using read_excel() function from the readxl package\n\nlibrary(readxl)\ndf &lt;- read_excel(\"data.xlsx\")\n\n4.- Using tibble() function from the tibble package\n\nlibrary(tibble)\ndf &lt;- tibble(x = c(1, 2, 3), y = c(\"a\", \"b\", \"c\"), z = c(TRUE, FALSE, TRUE))\n\nYou can check our other tutorials to handle data frames and start practicing. We also recommend these books from the website for you to go into more detail.\n\n\n\n\n  \n\n    \n      \n    \n    \n    \n  \n\n  \n\n    \n      \n    \n    \n    \n  \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "data_science/i2ds.html",
    "href": "data_science/i2ds.html",
    "title": "I2DS Tools for Data Science",
    "section": "",
    "text": "A workshop on tools for data science with R. Run by students of the course Introduction to Data Science (IDS) at the Hertie School, Berlin, Germany. All content produced and prepared by students of the Master of Data Science for Public Policy, the Master of Public Policy, and the Master of International Affairs.\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nI2DS Tools for Data Science Workshop 2021\n\n\nA workshop on tools for data science with R. Run by students of the course Introduction to Data Science (IDS) at the Hertie School, Berlin, Germany.\n\n\n\nI2DS\n\n\nR\n\n\nHertie School\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI2DS Tools for Data Science Workshop 2022\n\n\nA workshop on tools for data science with R. Run by students of the course Introduction to Data Science (IDS) at the Hertie School, Berlin, Germany.\n\n\n\nI2DS\n\n\nR\n\n\nHertie School\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "references/reference.html",
    "href": "references/reference.html",
    "title": "{{< fa solid scale-balanced >}} Ethics <span style='font-size: 18px;'>&#x2715;</span> Data Science {{< fa solid code-branch >}}",
    "section": "",
    "text": "Examples of using Observable with reactable in Quarto, based on the Observable JS Penguins example from the Quarto documentation.\nSource code: observable-reactable.qmd"
  },
  {
    "objectID": "references/reference.html#using-observable-inputs-to-filter-reactable",
    "href": "references/reference.html#using-observable-inputs-to-filter-reactable",
    "title": "{{< fa solid scale-balanced >}} Ethics <span style='font-size: 18px;'>&#x2715;</span> Data Science {{< fa solid code-branch >}}",
    "section": "Using Observable Inputs to filter reactable",
    "text": "Using Observable Inputs to filter reactable\n\nviewof billLengthMin = Inputs.range(\n  [32, 50], \n  { value: 35, step: 1, label: \"Bill length (min):\" }\n)\n\nviewof islands = Inputs.checkbox(\n  [\"Torgersen\", \"Biscoe\", \"Dream\"], \n  { value: [\"Torgersen\", \"Biscoe\"], label: \"Islands:\" }\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReactable.setFilter('tbl', 'bill_length', billLengthMin)\n\n\n\n\n\n\n\nReactable.setFilter('tbl', 'island', islands)"
  },
  {
    "objectID": "references/reference.html#using-reactable-to-filter-observable-charts",
    "href": "references/reference.html#using-reactable-to-filter-observable-charts",
    "title": "{{< fa solid scale-balanced >}} Ethics <span style='font-size: 18px;'>&#x2715;</span> Data Science {{< fa solid code-branch >}}",
    "section": "Using reactable to filter Observable charts",
    "text": "Using reactable to filter Observable charts\n\n// Create an Observable value that automatically tracks the table's filtered data\nfilteredData = Generators.observe(change =&gt; {\n  return Reactable.onStateChange('tbl-input', state =&gt; {\n    change(state.sortedData)\n  })\n})\n\n\n\n\n\n\n\n\n\n\n\n\nPenguin body mass by sex and species\n\nPlot.rectY(filteredData, \n  Plot.binX(\n    { y: \"count\" }, \n    { x: \"body_mass\", fill: \"species\", thresholds: 20 }\n  ))\n  .plot({\n    facet: {\n      data: filteredData,\n      x: \"sex\",\n      y: \"species\",\n      marginRight: 80\n    },\n    marks: [\n      Plot.frame(),\n    ]\n  }\n)"
  },
  {
    "objectID": "data_science/ds/example/index.html",
    "href": "data_science/ds/example/index.html",
    "title": "Your Document Title",
    "section": "",
    "text": "Your important content or announcement goes here."
  },
  {
    "objectID": "data_science/ds/example/index.html#important-section-title",
    "href": "data_science/ds/example/index.html#important-section-title",
    "title": "Your Document Title",
    "section": "",
    "text": "Your important content or announcement goes here."
  },
  {
    "objectID": "data_science/ds/example/index.html#subsection-title",
    "href": "data_science/ds/example/index.html#subsection-title",
    "title": "Your Document Title",
    "section": "Subsection Title",
    "text": "Subsection Title\nDetails about the subsection. This can include code snippets, explanations, or any relevant information.\n\n# R code goes here\n\n# Example\n\nx &lt;- 1:10\n\ny &lt;- x^2\n\nplot(x, y)"
  },
  {
    "objectID": "data_science/ds/social_media_scraping/index.html",
    "href": "data_science/ds/social_media_scraping/index.html",
    "title": "Introduction to Social Media Scraping",
    "section": "",
    "text": "NEWS\n\n\n\nWith the next Twitter regulations, The free package comes with rate-limited access to v2 tweet posting and media upload endpoints, with a posting limit of 1,500 tweets per month at the app level. If you want to access to posting tweets are up to 3,000 per month, and for reading, it’s up to 10,000 tweets per month, you will need to pay 100 USD per month. However, you can sill acces to databases and use this tutorial to analyze text in general."
  },
  {
    "objectID": "data_science/ds/social_media_scraping/index.html#loading-in-packages-and-authorizing-rtweet",
    "href": "data_science/ds/social_media_scraping/index.html#loading-in-packages-and-authorizing-rtweet",
    "title": "Introduction to Social Media Scraping",
    "section": "Loading in packages and authorizing rtweet",
    "text": "Loading in packages and authorizing rtweet\nSo the first thing we want to do is load in the packages we’ll be using to scrape and manipulate our data. The most important of those is rtweet, which is the one we’ll be using to interact with the Twitter API.\nIn order to scrape tweets, you’ll need a Twitter developer account and have to make a Twitter app. This is actually a pretty simple process (and won’t require any coding). Here’s a step-by-step guide\n\npacman::p_load(rtweet, tidyverse, ggplot2, utils, tm, SnowballC, caTools, \n               rpart, topicmodels, tidytext, wordcloud, lexicon, reshape2,\n               sentimentr)\n\nRunning the code above (without the #’s) will prompt a dialogue box to pop up on your screen asking you for a bearer token. You can find that on the Twitter developer page. I made the last two lines into comments so that this can be knit into HTML smoothly."
  },
  {
    "objectID": "data_science/ds/social_media_scraping/index.html#corpus",
    "href": "data_science/ds/social_media_scraping/index.html#corpus",
    "title": "Introduction to Social Media Scraping",
    "section": "Corpus",
    "text": "Corpus\nIn natural language processing (NLP), a corpus is a collection of written or spoken language that is used as a basis for analysis. A corpus can be made up of many different types of texts, including books, articles, speeches, social media posts, and more. Here, we will make a corpus of documents using just the full_text part of green tweets\n\ncorpus1 &lt;- Corpus(VectorSource(green$full_text))\n\nNow we need to clean our text a bit. Change to lower case and remove punctuation!\n\ncorpus1 &lt;- tm_map(corpus1, tolower)\ncorpus1 &lt;- tm_map(corpus1, removePunctuation)\n#We need to remove stop words to get meaningful results from this exercise. \n#We'll remove words like \"me\", \"is\", \"was\"\nstopwords(\"english\")[1:50]\n\n [1] \"i\"          \"me\"         \"my\"         \"myself\"     \"we\"        \n [6] \"our\"        \"ours\"       \"ourselves\"  \"you\"        \"your\"      \n[11] \"yours\"      \"yourself\"   \"yourselves\" \"he\"         \"him\"       \n[16] \"his\"        \"himself\"    \"she\"        \"her\"        \"hers\"      \n[21] \"herself\"    \"it\"         \"its\"        \"itself\"     \"they\"      \n[26] \"them\"       \"their\"      \"theirs\"     \"themselves\" \"what\"      \n[31] \"which\"      \"who\"        \"whom\"       \"this\"       \"that\"      \n[36] \"these\"      \"those\"      \"am\"         \"is\"         \"are\"       \n[41] \"was\"        \"were\"       \"be\"         \"been\"       \"being\"     \n[46] \"have\"       \"has\"        \"had\"        \"having\"     \"do\"        \n\ncorpus1 &lt;- tm_map(corpus1, removeWords, (stopwords(\"english\")))\n#We need to clean the words in the corpus further by \"stemming\" words\n#A word like \"understand\" and \"understands\" will both become \"understand\"\ncorpus1 &lt;- tm_map(corpus1, stemDocument)"
  },
  {
    "objectID": "data_science/ds/social_media_scraping/index.html#document-term-matrix",
    "href": "data_science/ds/social_media_scraping/index.html#document-term-matrix",
    "title": "Introduction to Social Media Scraping",
    "section": "Document Term Matrix",
    "text": "Document Term Matrix\nSo, a DTM is a way of representing a collection of text documents quantitativelythat allows us to do some cool stuff with them. It’s basically a matrix where the rows correspond to the documents in the collection, and the columns correspond to the unique words or terms that appear in the documents. Each cell in the matrix represents the frequency of a particular term in a particular document.\n\n#creates a document term matrix, which is necessary for building a topic model\nDTM1 &lt;- DocumentTermMatrix(corpus1)\n#Here we can see the most frequently used terms\nfrequent_ge_20 &lt;- findFreqTerms(DTM1, lowfreq = 100)\nfrequent_ge_20\n\n [1] \"amp\"       \"green\"     \"like\"      \"store\"     \"day\"       \"…\"        \n [7] \"’s\"        \"light\"     \"one\"       \"year\"      \"figur\"     \"red\"      \n[13] \"buddha\"    \"fasc1nat\"  \"four\"      \"f…\"        \"philippin\" \"pray\"     \n[19] \"purchas\"   \"spent\"     \"woman\""
  },
  {
    "objectID": "data_science/ds/social_media_scraping/index.html#lets-create-the-topic-model-well-start-with-5-topics",
    "href": "data_science/ds/social_media_scraping/index.html#lets-create-the-topic-model-well-start-with-5-topics",
    "title": "Introduction to Social Media Scraping",
    "section": "Let’s create the topic model! We’ll start with 5 topics",
    "text": "Let’s create the topic model! We’ll start with 5 topics\nThe code snippet performs topic modeling on a corpus of text data represented as a Document-Term Matrix (DTM) using the Latent Dirichlet Allocation (LDA) algorithm. The LDA() function from the topic models package is used to create a model with 7 topics and a specified random seed for reproducibility. The resulting model is stored in the green_lda1 object.\n\n#Perform LDA topic modeling on a Document-Term Matrix (DTM) with 7 topics\ngreen_lda1 &lt;- LDA(DTM1, k = 7, control = list(seed = 1234))\n\n#Print the model summary\ngreen_lda1\n\nA LDA_VEM topic model with 7 topics.\n\n#Convert the model's beta matrix to a tidy format\ngreen_topics1 &lt;- tidy(green_lda1, matrix = \"beta\")\n\ngreen_top_terms1 &lt;- green_topics1 %&gt;%\n  group_by(topic) %&gt;% #Group the terms by topic\n  slice_max(beta, n = 10) %&gt;% #Top 10 terms with the highest probabilities\n  ungroup() %&gt;% #Remove the grouping attribute from the data frame\n  arrange(topic, -beta) #Sort the data frame by topic index and term probability"
  },
  {
    "objectID": "data_science/ds/social_media_scraping/index.html#word-cloud-of-biden-tweets",
    "href": "data_science/ds/social_media_scraping/index.html#word-cloud-of-biden-tweets",
    "title": "Introduction to Social Media Scraping",
    "section": "Word Cloud of Biden Tweets",
    "text": "Word Cloud of Biden Tweets\n\nCode# Create custom color palette\nmy_palette &lt;- brewer.pal(8, \"Dark2\")\n\n# Create word cloud with larger font size and custom layout\nwordcloud(words_data2$word, \n          words_data2$n, \n          max.words = 200, \n          colors = my_palette, \n          scale = c(5, 0.3),\n          random.order = FALSE,\n          rot.per = 0.25,\n          random.color = TRUE,\n          main = \"Word Cloud of Biden Tweets\")"
  },
  {
    "objectID": "data_science/ds/social_media_scraping/index.html#comparison-cloud-of-biden-tweets-by-sentiment",
    "href": "data_science/ds/social_media_scraping/index.html#comparison-cloud-of-biden-tweets-by-sentiment",
    "title": "Introduction to Social Media Scraping",
    "section": "Comparison Cloud of Biden Tweets by Sentiment",
    "text": "Comparison Cloud of Biden Tweets by Sentiment\nNow let’s make a word cloud from those tweets but highlight which words are positive and which are negative\n\n#Select and tokenize words\nwords_data &lt;- biden_tweets %&gt;% \n  select(text) %&gt;%\n  unnest_tokens(word, text) \n\n#Get sentiment scores for each word using the bing lexicon\nsentiment_scores &lt;- words_data2 %&gt;%\n  inner_join(get_sentiments(\"bing\"))\n\n#Count the number of words in each sentiment category\nsentiment_counts &lt;- sentiment_scores %&gt;%\n  count(sentiment, sort = TRUE)\n\n#Create a list of profanity words to remove from the dataset\nprofanity_list &lt;- unique(tolower(lexicon::profanity_alvarez))\n\n#Filter out stop words and profanity words from the dataset\nfiltered_words_data &lt;- words_data %&gt;%\n  filter(!word %in% c('https', 't.co', 'he\\'s', 'i\\'m', 'it\\'s', profanity_list))\n\n#Get sentiment scores for each filtered word using the bing lexicon\nfiltered_sentiment_scores &lt;- filtered_words_data %&gt;%\n  inner_join(get_sentiments(\"bing\"))"
  },
  {
    "objectID": "data_science/ds/social_media_scraping/index.html#cloud",
    "href": "data_science/ds/social_media_scraping/index.html#cloud",
    "title": "Introduction to Social Media Scraping",
    "section": "Cloud",
    "text": "Cloud\n\nCode#Count the number of filtered words with each sentiment score\nword_sentiment_counts &lt;- filtered_sentiment_scores %&gt;%\n  count(word, sentiment, sort = TRUE) %&gt;%\n  acast(word ~ sentiment, value.var = \"n\", fill = 0)\n\n#Create a comparison cloud using the word and sentiment counts\ncomparison.cloud(word_sentiment_counts, \n                  colors = c(\"red\", \"blue\"),\n                  max.words = Inf)"
  },
  {
    "objectID": "data_science/I2DS/i2ds2022/index.html",
    "href": "data_science/I2DS/i2ds2022/index.html",
    "title": "I2DS Tools for Data Science Workshop 2022",
    "section": "",
    "text": "The Introduction to Data Science (IDS) course at the Hertie School in Berlin, Germany, is designed to provide students with a comprehensive understanding of data science principles, techniques, and tools. As part of the course curriculum, students actively engage in organizing workshops and activities to enhance their practical skills and share their knowledge with others.\n \nThese student-led activities, such as the workshop on Tools for Data Science with R, serve as valuable opportunities for participants to gain hands-on experience and learn directly from their peers. The I2DS course encourages students to take the lead in organizing and delivering these workshops, fostering a collaborative learning environment.\n\n\n\n Go to I2DS Tools 2022"
  },
  {
    "objectID": "ethics/ethics.html",
    "href": "ethics/ethics.html",
    "title": "{{< fa solid scale-balanced >}} Ethics <span style='font-size: 18px;'>&#x2715;</span> Data Science {{< fa solid code-branch >}}",
    "section": "",
    "text": "Ethics develops skills of qualitative analysis, reasoning, and communication. It is a key component of good decision-making and leadership. The goal of ethics in data science is to identify ethical issues, and articulate them, to draw on ethical vocabulary to analyze options, and to evaluate and communicate arguments for and against a decision.\nThis section is dedicated to exploring the complex ethical questions that arise in the field of data science. It’s here that we confront the challenges of balancing innovation with privacy, algorithmic fairness with efficiency, and the pursuit of insight with respect for individual rights. Our goal is to provide you with resources and thought-provoking content that illuminate the ethical dimensions of working with data."
  },
  {
    "objectID": "ethics/ethics.html#ethics",
    "href": "ethics/ethics.html#ethics",
    "title": "{{< fa solid scale-balanced >}} Ethics <span style='font-size: 18px;'>&#x2715;</span> Data Science {{< fa solid code-branch >}}",
    "section": "Ethics",
    "text": "Ethics\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nThe Pathology of Privacy\n\n\nWhat’s the state of privacy today? Protecting privacy is a collective effort. It follows that consent is over-rated and ignorance is rational.\n\n\n\nFoundational Concept\n\n\nPrivacy\n\n\nData Collection\n\n\n\n\n\n\nFeb 26, 2024\n\n\nJohannes Himmelreich\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\nApproaches\n\n\nThree approaches for Data Science Ethics — and why you should think about how you approach the ethics of data science\n\n\n\nFoundational Concepts\n\n\nBias\n\n\nMethodology\n\n\n\n\n\n\nFeb 22, 2024\n\n\nJohannes Himmelreich\n\n\n17 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "ethics/ethics/privacy/index.html",
    "href": "ethics/ethics/privacy/index.html",
    "title": "Privacy",
    "section": "",
    "text": "Main idea\n\n\n\nYou have no idea what privacy is. But you still know what you need to do to protect it."
  },
  {
    "objectID": "ethics/ethics/privacy/index.html#information-problems",
    "href": "ethics/ethics/privacy/index.html#information-problems",
    "title": "Privacy",
    "section": "Information problems",
    "text": "Information problems\nSometimes, you know how limited your privacy is. You remember there were privacy settings in that app that you just couldn’t bother to check. And you know that online advertisers track you. And you know that you signed something at your doctor’s office about sharing your patient data.\nBut often, privacy today is violated unwittingly. For one, it’s basically impossible to be aware of all the ways in which your data is collected, analyzed, and shared. Similarly, nobody can possibly read all the terms and conditions and privacy policies that they are agreeing to.\nIn short, there is an information problem: Individuals don’t know — and, to some extent, cannot know — all they need to know to safeguard their privacy. And, arguably, we shouldn’t spend much of our time and mental energy on protecting our privacy."
  },
  {
    "objectID": "ethics/ethics/privacy/index.html#choice-problems",
    "href": "ethics/ethics/privacy/index.html#choice-problems",
    "title": "Privacy",
    "section": "Choice problems",
    "text": "Choice problems\nThe second part of the state of individual privacy today has to do with choices. We cannot throw away all of our devices, like Ron Swanson does. The options that we have to protect our privacy, even when we are aware of them, is practically limited. Metadata collection that is required by law is hard to opt out of. Even tech-savvy individuals are outgunned by the professional data, surveillance, and security industry.\nAnd then, we also have a cognitive system that is easy to exploit. As such, websites and apps use various forms of deception to get you to do things.1 You end up subscribing to a service without knowing, agreeing to terms without having read them, or clicking “Continue” because the other button, “No,” was made hard to see.\n\n\n\n\n\nDeceptive design\n\n\nIn sum, privacy is inadvertently infringed on a regular basis in two respects\n\nIndividuals hold false beliefs about the state of their privacy and the options in front of them\nIndividuals have limited options to safeguard their privacy\n\nThese two points are practically necessary. That is, given how things are, these are two starting points that we cannot change. You can only read so many terms and conditions, if you can read any, and there is only so much you can do."
  },
  {
    "objectID": "ethics/ethics/privacy/index.html#footnotes",
    "href": "ethics/ethics/privacy/index.html#footnotes",
    "title": "Privacy",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee https://www.deceptive.design↩︎"
  },
  {
    "objectID": "for_instructors/guide/ds/index.html",
    "href": "for_instructors/guide/ds/index.html",
    "title": "Data Science",
    "section": "",
    "text": "Your important content or announcement goes here."
  },
  {
    "objectID": "for_instructors/guide/ds/index.html#important-section-title",
    "href": "for_instructors/guide/ds/index.html#important-section-title",
    "title": "Data Science",
    "section": "",
    "text": "Your important content or announcement goes here."
  },
  {
    "objectID": "for_instructors/guide/ds/index.html#subsection-title",
    "href": "for_instructors/guide/ds/index.html#subsection-title",
    "title": "Data Science",
    "section": "Subsection Title",
    "text": "Subsection Title\nDetails about the subsection. This can include code snippets, explanations, or any relevant information.\n\n# R code goes here\n\n# Example\n\nx &lt;- 1:10\n\ny &lt;- x^2\n\nplot(x, y)\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nCite this page: Lehmann, L. (2023, April 18). Introduction to Social Media Scraping. Hertie Coding Club. URL"
  },
  {
    "objectID": "data_science/ds/objects/index.html#background-and-prerequisites",
    "href": "data_science/ds/objects/index.html#background-and-prerequisites",
    "title": "Bias in AI: detection and mitigation",
    "section": "Background and Prerequisites",
    "text": "Background and Prerequisites\nThis tutorial is designed for users with a basic understanding of Python and Deep Learning. Users should have a foundational understanding of key concepts in machine learning and neural networks. Familiarity with Python is essential. Additionally, a grasp of linear algebra and calculus will be beneficial for understanding the mathematical underpinnings of deep learning algorithms.\n\nA solid understanding of model training is crucial, as well as knowledge of common machine learning libraries such as Keras and scikit-learn. Users should also be aware of the ethical and policy considerations surrounding machine learning applications, particularly in relation to bias and fairness.\nLastly, a conceptual understanding of how neural networks operate, including layers, activation functions, and backpropagation, will enhance the learning experience of the user. Overall, a basic background in machine learning fundamentals will help users to engage more effectively with our tutorial.\n\n\n!pip install pandas numpy matplotlib\n!pip install Aequitas\n!pip install keras_tuner\n!pip install aif360\n!pip install BlackBoxAuditing\n!pip install tensorflow\n\n\n# Data visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"white\", palette=\"muted\", color_codes=True, context=\"talk\")\nfrom IPython import display\n\n# Data manipulation\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\n\n# Aequitas library used to audit models for discrimination and bias\nfrom aequitas.group import Group\nfrom aequitas.bias import Bias\nfrom aequitas.fairness import Fairness\nfrom aequitas.plotting import Plot\nimport matplotlib.pyplot as plt\nimport warnings; warnings.simplefilter('ignore')\n\n# Machine and deep learning libraries\nimport tensorflow as tf\nfrom keras.layers import Input, Dense, Dropout\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nimport keras_tuner as kt\nfrom keras import Input, Model\nfrom keras.layers import Dense, Dropout\nfrom keras.optimizers import Adam\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n\n# AI fairness library\nfrom aif360.algorithms.preprocessing import DisparateImpactRemover\nfrom aif360.datasets import StandardDataset as Dataset\nfrom aif360.metrics import BinaryLabelDatasetMetric\nfrom aif360.algorithms.postprocessing.reject_option_classification import RejectOptionClassification\nfrom aif360.algorithms.preprocessing.reweighing import Reweighing\nfrom collections import OrderedDict\nfrom aif360.metrics import ClassificationMetric"
  },
  {
    "objectID": "data_science/ds/objects/index.html#data-download",
    "href": "data_science/ds/objects/index.html#data-download",
    "title": "Bias in AI: detection and mitigation",
    "section": "1.1 Data Download",
    "text": "1.1 Data Download\n\n# Load the data\n\ndf_compas_aeq = pd.read_csv(\"https://raw.githubusercontent.com/dssg/aequitas/master/examples/data/compas_for_aequitas.csv\")\ndf_compas_aeq.head()\n\n\n\n\n\n\n\n\n\nentity_id\nscore\nlabel_value\nrace\nsex\nage_cat\n\n\n\n\n0\n1\n0.0\n0\nOther\nMale\nGreater than 45\n\n\n1\n3\n0.0\n1\nAfrican-American\nMale\n25 - 45\n\n\n2\n4\n0.0\n1\nAfrican-American\nMale\nLess than 25\n\n\n3\n5\n1.0\n0\nAfrican-American\nMale\nLess than 25\n\n\n4\n6\n0.0\n0\nOther\nMale\n25 - 45"
  },
  {
    "objectID": "data_science/ds/objects/index.html#exploratory-data-visualization",
    "href": "data_science/ds/objects/index.html#exploratory-data-visualization",
    "title": "Bias in AI: detection and mitigation",
    "section": "Exploratory Data Visualization",
    "text": "Exploratory Data Visualization\n\nDistribution of Defendants by Demographics (Race, Age, Sex) and Risk Scores\n\nAs a first step, we are exploring the distribution of our defendant data with regards to demographic characteristics and the calculated risk scores. As we can see, African-Americans, Caucasians, males, and defendants aged 25-45 are the subgroups that are highly represented in the data. Additionally, we can already see from the plots that African-Americans and defendants aged under 25 are the only subgroups where the majority has been assigned a high risk score.\n\n\nCode\nReds_palette = sns.diverging_palette(204, 0, n=2)\n\n# Create a figure with 3 subplots (3 rows, 1 column)\nfig, axes = plt.subplots(3, 1, figsize=(8, 16))\n\n# race\nby_race = sns.countplot(\n    ax=axes[0],\n    x=\"race\",\n    hue=\"score\",\n    data=df_compas_aeq,\n    palette=Reds_palette\n)\n\naxes[0].set_title(\"Distribution of Defendants by Race and Risk Score (Decile)\")\naxes[0].set_xlabel(\"Race\")\naxes[0].set_ylabel(\"Count\")\naxes[0].legend(loc='upper right', title='Risk Score Decile')\naxes[0].grid(True, linestyle='--', linewidth=0.5)\naxes[0].tick_params(axis='x', rotation=45)\n\n# sex\nby_sex = sns.countplot(\n    ax=axes[1],\n    x=\"sex\",\n    hue=\"score\",\n    data=df_compas_aeq,\n    palette=Reds_palette\n)\n\n# Add title and labels\naxes[1].set_title(\"Distribution of Defendants by Sex and Risk Score (Decile)\")\naxes[1].set_xlabel(\"Sex\")\naxes[1].set_ylabel(\"Count\")\n\n# sex\naxes[1].legend(loc='upper right', title='Risk Score')\naxes[1].grid(True, linestyle='--', linewidth=0.5)\naxes[1].tick_params(axis='x', rotation=45)\n\n# Create countplot for age\nby_age = sns.countplot(\n    ax=axes[2],\n    x=\"age_cat\",\n    hue=\"score\",\n    data=df_compas_aeq,\n    palette=Reds_palette\n)\n\naxes[2].set_title(\"Distribution of Defendants by Age and Risk Score (Decile)\")\naxes[2].set_xlabel(\"Age Category\")\naxes[2].set_ylabel(\"Count\")\n\naxes[2].legend(loc='upper right', title='Risk Score')\n\naxes[2].grid(True, linestyle='--', linewidth=0.5)\n\naxes[2].tick_params(axis='x', rotation=45)\n\nplt.tight_layout()\n\nplt.show()"
  },
  {
    "objectID": "data_science/ds/objects/index.html#introducing-the-aequitas-library",
    "href": "data_science/ds/objects/index.html#introducing-the-aequitas-library",
    "title": "Bias in AI: detection and mitigation",
    "section": "Introducing the Aequitas-Library",
    "text": "Introducing the Aequitas-Library\nAfter eyeballing our data set and noticing that there might be some fairness issues, we can now use the Aequitas library to calculate common metrics that indicate biases in subgroups. More specifically, we are using the library’s Group() class that evaluates biases across all demographic subgroups in the dataset. Note here that the library requires the input data to have columns named “score” and “label_value”. These columns are by default used to calculate the bias metrics.\nIn order to use Aequitas for your purposes, you should rename the columns that you want to check for potential biases to “score” and “label_value”. Additionally, at least one column needs to include grouping information (in our example, several demographic variables). ID variables as entity_id are by default not treated as grouping variables.\nThe following code chunk calculates these metrices for all demographic subgroups using the get_crosstabs function, based on the risk score and the label_value, which indicates the recidivism.\n\n\nCode\ng = Group()\nxtab, _ = g.get_crosstabs(df_compas_aeq)\nxtab\n\n\n\n\n\n\n\n\n\n\nmodel_id\nscore_threshold\nk\nattribute_name\nattribute_value\naccuracy\ntpr\ntnr\nfor\nfdr\n...\npprev\nfp\nfn\ntn\ntp\ngroup_label_pos\ngroup_label_neg\ngroup_size\ntotal_entities\nprev\n\n\n\n\n0\n0\nbinary 0/1\n3317\nrace\nAfrican-American\n0.638258\n0.720147\n0.551532\n0.349540\n0.370285\n...\n0.588203\n805\n532\n990\n1369\n1901\n1795\n3696\n7214\n0.514340\n\n\n1\n0\nbinary 0/1\n3317\nrace\nAsian\n0.843750\n0.666667\n0.913043\n0.125000\n0.250000\n...\n0.250000\n2\n3\n21\n6\n9\n23\n32\n7214\n0.281250\n\n\n2\n0\nbinary 0/1\n3317\nrace\nCaucasian\n0.669927\n0.522774\n0.765457\n0.288125\n0.408665\n...\n0.348003\n349\n461\n1139\n505\n966\n1488\n2454\n7214\n0.393643\n\n\n3\n0\nbinary 0/1\n3317\nrace\nHispanic\n0.660911\n0.443966\n0.785185\n0.288591\n0.457895\n...\n0.298273\n87\n129\n318\n103\n232\n405\n637\n7214\n0.364207\n\n\n4\n0\nbinary 0/1\n3317\nrace\nNative American\n0.777778\n0.900000\n0.625000\n0.166667\n0.250000\n...\n0.666667\n3\n1\n5\n9\n10\n8\n18\n7214\n0.555556\n\n\n5\n0\nbinary 0/1\n3317\nrace\nOther\n0.665782\n0.323308\n0.852459\n0.302013\n0.455696\n...\n0.209549\n36\n90\n208\n43\n133\n244\n377\n7214\n0.352785\n\n\n6\n0\nbinary 0/1\n3317\nsex\nFemale\n0.653763\n0.608434\n0.678930\n0.242537\n0.487310\n...\n0.423656\n288\n195\n609\n303\n498\n897\n1395\n7214\n0.356989\n\n\n7\n0\nbinary 0/1\n3317\nsex\nMale\n0.653721\n0.629132\n0.675799\n0.330100\n0.364637\n...\n0.468465\n994\n1021\n2072\n1732\n2753\n3066\n5819\n7214\n0.473105\n\n\n8\n0\nbinary 0/1\n3317\nage_cat\n25 - 45\n0.647846\n0.626257\n0.666216\n0.323112\n0.385135\n...\n0.468240\n741\n706\n1479\n1183\n1889\n2220\n4109\n7214\n0.459723\n\n\n9\n0\nbinary 0/1\n3317\nage_cat\nGreater than 45\n0.704315\n0.427711\n0.832096\n0.241117\n0.459391\n...\n0.250000\n181\n285\n897\n213\n498\n1078\n1576\n7214\n0.315990\n\n\n10\n0\nbinary 0/1\n3317\nage_cat\nLess than 25\n0.617397\n0.739583\n0.458647\n0.424528\n0.360360\n...\n0.653368\n360\n225\n305\n639\n864\n665\n1529\n7214\n0.565075\n\n\n\n\n11 rows × 27 columns\n\n\n\n\n\nAdditionally, we can use list_absolute_metrics() for an improved overview grouped by the demographics and with rounded values for the metrics.\n\n\n\nCode\nabsolute_metrics = g.list_absolute_metrics(xtab)\nxtab[['attribute_name', 'attribute_value'] + absolute_metrics].round(2)\n\n\n\n\n\n\n\n\n\n\nattribute_name\nattribute_value\naccuracy\ntpr\ntnr\nfor\nfdr\nfpr\nfnr\nnpv\nprecision\nppr\npprev\nprev\n\n\n\n\n0\nrace\nAfrican-American\n0.64\n0.72\n0.55\n0.35\n0.37\n0.45\n0.28\n0.65\n0.63\n0.66\n0.59\n0.51\n\n\n1\nrace\nAsian\n0.84\n0.67\n0.91\n0.12\n0.25\n0.09\n0.33\n0.88\n0.75\n0.00\n0.25\n0.28\n\n\n2\nrace\nCaucasian\n0.67\n0.52\n0.77\n0.29\n0.41\n0.23\n0.48\n0.71\n0.59\n0.26\n0.35\n0.39\n\n\n3\nrace\nHispanic\n0.66\n0.44\n0.79\n0.29\n0.46\n0.21\n0.56\n0.71\n0.54\n0.06\n0.30\n0.36\n\n\n4\nrace\nNative American\n0.78\n0.90\n0.62\n0.17\n0.25\n0.38\n0.10\n0.83\n0.75\n0.00\n0.67\n0.56\n\n\n5\nrace\nOther\n0.67\n0.32\n0.85\n0.30\n0.46\n0.15\n0.68\n0.70\n0.54\n0.02\n0.21\n0.35\n\n\n6\nsex\nFemale\n0.65\n0.61\n0.68\n0.24\n0.49\n0.32\n0.39\n0.76\n0.51\n0.18\n0.42\n0.36\n\n\n7\nsex\nMale\n0.65\n0.63\n0.68\n0.33\n0.36\n0.32\n0.37\n0.67\n0.64\n0.82\n0.47\n0.47\n\n\n8\nage_cat\n25 - 45\n0.65\n0.63\n0.67\n0.32\n0.39\n0.33\n0.37\n0.68\n0.61\n0.58\n0.47\n0.46\n\n\n9\nage_cat\nGreater than 45\n0.70\n0.43\n0.83\n0.24\n0.46\n0.17\n0.57\n0.76\n0.54\n0.12\n0.25\n0.32\n\n\n10\nage_cat\nLess than 25\n0.62\n0.74\n0.46\n0.42\n0.36\n0.54\n0.26\n0.58\n0.64\n0.30\n0.65\n0.57\n\n\n\n\n\n\n\n\n\nNext, we can use the information on the metrics that have been calculated by the previous chunk to plot the present biases. For that purpose, the Plot() class is used and stored in a variable. Afterwards, this variable can be used to plot the metrics of interest. The next code chunk exemplarily plots the false positive rate for all subgroups. In the context of our data, false positive cases are present when defendants are classified high risk although they did not recidivate. As we can see from the plot below, these cases are especially present amoung younger as well as among African- and Native Americans.\nAdditionally, the colors by default indicate how many respondents are included in the respective subgroup. The exact number can also be retrieved from the bar labels. Referring to the group sizes, you can see that the two races with they highest FPR are of significantly different size: While there are only 18 Native Americans included in our data, a total of nearly 3,700 African-American defendants are present.\n\n\nCode\naqp = Plot()\nfpr = aqp.plot_group_metric(xtab, 'fpr')\n\n\n\n\n\n\n\n\n\n\nFor better readability, and when only interested in the rates rather then the absolute numbers, we can switch the axes and rotate the x-axis labels:\n\n\n\nCode\nxtab_df = xtab[['attribute_name', 'attribute_value'] + absolute_metrics].round(2).set_index(['attribute_name', 'attribute_value'])\nxtab_df = xtab_df.reset_index()\n\n# Create a figure for the plot\nplt.figure(figsize=(9, 6))\n\n# Create a bar plot for FPR\nax = sns.barplot(x='attribute_value', y='fpr', hue='attribute_name', data=xtab_df, palette='coolwarm', dodge=True)\nax.set_xticklabels(ax.get_xticklabels(), rotation=60, ha='right')\n\n\n# Add title and labels\nplt.title('False Positive Rate (FPR) by Attribute')\nplt.xlabel('Attribute Value')\nplt.ylabel('FPR')\n\n# Rotate x-axis labels for better readability\nplt.xticks(rotation=60)\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\nBesides the FPR, the Aequitas package calculated several further helpful metrices that illustrate biases. In the following code chunk, we present four metrices in total:\n\nThe Predicted Positive Rate (PPR): Proportion of positively predicted defendants\nPredictive Prevalence (PPrev): Positive predicition at higher prevalence in relation to group size\nFalse Negative Rate: Negative risk predictions, positive score\nFalse Positive Rate: Positive risk predicition, negative score\n\nWe can see that African-Americans and males are largely predicted as having a high-risk score (PPR), which also holds true when considering group size (PPrev), although here, also younger defendants are highly represented. We already discussed FPR, showing that African-Americans are often misclassified as high risk. Looking at the FNR, we can see that African-Americans rather rarely are misclassified as low-risk, while this seems to be especially true with older defendants.\n\n\nCode\n# Plot\nfig, axes = plt.subplots(4, 1, figsize=(8, 30))  # 4 rows, 1 column\npalette = sns.color_palette(\"ocean_r\", n_colors=3)\n\n# PPR Plot in the first row (0)\nsns.barplot(x='attribute_value', y='ppr', hue='attribute_name', data=xtab_df, palette=palette, ax=axes[0])\naxes[0].set_title('Positive Predictive Rate (PPR) by Attribute')\naxes[0].set_xlabel('Attribute Value')\naxes[0].set_ylabel('PPR')\naxes[0].tick_params(axis='x', rotation=90)\naxes[0].legend(loc='upper right', title='Category')\n\n# PPrev Plot in the second row (1)\nsns.barplot(x='attribute_value', y='pprev', hue='attribute_name', data=xtab_df, palette=palette, ax=axes[1])\naxes[1].set_title('Predictive Prevalence (PPrev) by Attribute')\naxes[1].set_xlabel('Attribute Value')\naxes[1].set_ylabel('PPrev')\naxes[1].tick_params(axis='x', rotation=90)\naxes[1].legend(loc='upper center', title='Category')\n\n# FNR Plot in the third row (2)\nsns.barplot(x='attribute_value', y='fnr', hue='attribute_name', data=xtab_df, palette=palette, ax=axes[2])\naxes[2].set_title('False Negative Rate (FNR) by Attribute')\naxes[2].set_xlabel('Attribute Value')\naxes[2].set_ylabel('FNR')\naxes[2].tick_params(axis='x', rotation=90)\naxes[2].legend(loc='upper left', title='Category')\n\n# FPR Plot in the fourth row (3)\nsns.barplot(x='attribute_value', y='fpr', hue='attribute_name', data=xtab_df, palette=palette, ax=axes[3])\naxes[3].set_title('False Positive Rate (FPR) by Attribute')\naxes[3].set_xlabel('Attribute Value')\naxes[3].set_ylabel('FPR')\naxes[3].tick_params(axis='x', rotation=90)\naxes[3].legend(loc='upper center', title='Category')\n\n# Adjust the layout\nplt.tight_layout()\n\n# Show the combined plot\nplt.show()"
  },
  {
    "objectID": "data_science/ds/objects/index.html#distribution-of-defendants-by-demographics-and-recidivism",
    "href": "data_science/ds/objects/index.html#distribution-of-defendants-by-demographics-and-recidivism",
    "title": "Bias in AI: detection and mitigation",
    "section": "Distribution of Defendants by Demographics and Recidivism",
    "text": "Distribution of Defendants by Demographics and Recidivism\nNext, we are looking at the same demographic subgroups and whether the defendants actually committed crime again. We can already see, that there seems to be a mismatch between the assigned risk scores and the recidivism patterns.\n\n\nCode\ncoolwarm_two_colors = sns.color_palette(\"coolwarm\", n_colors=2)\ncoolwarm_palette = sns.color_palette(\"coolwarm\", as_cmap=True)\n\n\n# Create a figure with 3 subplots (3 rows, 1 column)\nfig, axes = plt.subplots(3, 1, figsize=(8, 16))\n\n# Create countplot for race\nlabel_by_race = sns.countplot(\n    ax=axes[0],\n    x=\"race\",\n    hue=\"label_value\",\n    data=df_compas_aeq,\n    palette=coolwarm_two_colors\n)\n\n# Add title and labels for race\naxes[0].set_title(\"Levels of recidivism by Race\")\naxes[0].set_xlabel(\"Race\")\naxes[0].set_ylabel(\"Count\")\naxes[0].grid(True, linestyle='--', linewidth=0.5)\naxes[0].legend(loc='upper right', title='Recidivism')\naxes[0].tick_params(axis='x', rotation=45)\n\n# Create countplot for sex\nlabel_by_sex = sns.countplot(\n    ax=axes[1],\n    x=\"sex\",\n    hue=\"label_value\",\n    data=df_compas_aeq,\n    palette=coolwarm_two_colors\n)\n\n# Add title and labels for sex\naxes[1].set_title(\"Levels of recidivism by Sex\")\naxes[1].set_xlabel(\"Sex\")\naxes[1].set_ylabel(\"Count\")\naxes[1].grid(True, linestyle='--', linewidth=0.5)\naxes[1].legend(loc='upper right', title='Recidivism')\naxes[1].tick_params(axis='x', rotation=45)\n\n# Create countplot for age category\nlabel_by_age = sns.countplot(\n    ax=axes[2],\n    x=\"age_cat\",\n    hue=\"label_value\",\n    data=df_compas_aeq,\n    palette=coolwarm_two_colors\n)\n\n# Add title and labels for age category\naxes[2].set_title(\"Levels of recidivism by Age Category\")\naxes[2].set_xlabel(\"Age Category\")\naxes[2].set_ylabel(\"Count\")\naxes[2].grid(True, linestyle='--', linewidth=0.5)\naxes[2].legend(loc='upper right', title='Recidivism')\n\n# Adjust layout\nplt.tight_layout()\n\n# Display the plot\nplt.show()"
  },
  {
    "objectID": "data_science/ds/objects/index.html#calculating-the-bias-using-disparity",
    "href": "data_science/ds/objects/index.html#calculating-the-bias-using-disparity",
    "title": "Bias in AI: detection and mitigation",
    "section": "Calculating the bias using disparity",
    "text": "Calculating the bias using disparity\nLastly, the Aequitas package lets us compare subgroups by calculating disparaties as a ratio of the desired metric.\nFor example, the disparity of the false positive ratio for black and white defendants can be calculated like this:\n\\(\\text{Disparity}_{FPR} = \\frac{FPR{\\text{black}}}{FPR{\\text{white}}}\\)\nTo easily calculate the disparity values for all metrices, we use the Bias() class of the Aquitas package and calculate a confusion matrix. Note that for each subgroup, you will need to assign a reference category. Here, we chose to compare all included subgroups to Caucasians, males, and defendants aged 25-45.\n\n\nCode\nb = Bias()\nbdf = b.get_disparity_predefined_groups(xtab, original_df=df_compas_aeq, ref_groups_dict={'race':'Caucasian', 'sex':'Male', 'age_cat':'25 - 45'}, alpha=0.05, mask_significance=True)\nbdf.style\n\n\n\n\n\n\n\n\n \nmodel_id\nscore_threshold\nk\nattribute_name\nattribute_value\naccuracy\ntpr\ntnr\nfor\nfdr\nfpr\nfnr\nnpv\nprecision\npp\npn\nppr\npprev\nfp\nfn\ntn\ntp\ngroup_label_pos\ngroup_label_neg\ngroup_size\ntotal_entities\nprev\nppr_disparity\npprev_disparity\nprecision_disparity\nfdr_disparity\nfor_disparity\nfpr_disparity\nfnr_disparity\ntpr_disparity\ntnr_disparity\nnpv_disparity\nppr_ref_group_value\npprev_ref_group_value\nprecision_ref_group_value\nfdr_ref_group_value\nfor_ref_group_value\nfpr_ref_group_value\nfnr_ref_group_value\ntpr_ref_group_value\ntnr_ref_group_value\nnpv_ref_group_value\n\n\n\n\n0\n0\nbinary 0/1\n3317\nrace\nAfrican-American\n0.638258\n0.720147\n0.551532\n0.349540\n0.370285\n0.448468\n0.279853\n0.650460\n0.629715\n2174\n1522\n0.655412\n0.588203\n805\n532\n990\n1369\n1901\n1795\n3696\n7214\n0.514340\n2.545667\n1.690224\n1.064904\n0.906085\n1.213154\n1.912093\n0.586416\n1.377549\n0.720526\n0.913728\nCaucasian\nCaucasian\nCaucasian\nCaucasian\nCaucasian\nCaucasian\nCaucasian\nCaucasian\nCaucasian\nCaucasian\n\n\n1\n0\nbinary 0/1\n3317\nrace\nAsian\n0.843750\n0.666667\n0.913043\n0.125000\n0.250000\n0.086957\n0.333333\n0.875000\n0.750000\n8\n24\n0.002412\n0.250000\n2\n3\n21\n6\n9\n23\n32\n7214\n0.281250\n0.009368\n0.718384\n1.268317\n0.611748\n0.433839\n0.370749\n0.698482\n1.275248\n1.192808\n1.229148\nCaucasian\nCaucasian\nCaucasian\nCaucasian\nCaucasian\nCaucasian\nCaucasian\nCaucasian\nCaucasian\nCaucasian\n\n\n2\n0\nbinary 0/1\n3317\nrace\nCaucasian\n0.669927\n0.522774\n0.765457\n0.288125\n0.408665\n0.234543\n0.477226\n0.711875\n0.591335\n854\n1600\n0.257462\n0.348003\n349\n461\n1139\n505\n966\n1488\n2454\n7214\n0.393643\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\nCaucasian\nCaucasian\nCaucasian\nCaucasian\nCaucasian\nCaucasian\nCaucasian\nCaucasian\nCaucasian\nCaucasian\n\n\n3\n0\nbinary 0/1\n3317\nrace\nHispanic\n0.660911\n0.443966\n0.785185\n0.288591\n0.457895\n0.214815\n0.556034\n0.711409\n0.542105\n190\n447\n0.057281\n0.298273\n87\n129\n318\n103\n232\n405\n637\n7214\n0.364207\n0.222482\n0.857099\n0.916748\n1.120464\n1.001616\n0.915887\n1.165140\n0.849249\n1.025773\n0.999346\nCaucasian\nCaucasian\nCaucasian\nCaucasian\nCaucasian\nCaucasian\nCaucasian\nCaucasian\nCaucasian\nCaucasian\n\n\n4\n0\nbinary 0/1\n3317\nrace\nNative American\n0.777778\n0.900000\n0.625000\n0.166667\n0.250000\n0.375000\n0.100000\n0.833333\n0.750000\n12\n6\n0.003618\n0.666667\n3\n1\n5\n9\n10\n8\n18\n7214\n0.555556\n0.014052\n1.915691\n1.268317\n0.611748\n0.578453\n1.598854\n0.209544\n1.721584\n0.816506\n1.170618\nCaucasian\nCaucasian\nCaucasian\nCaucasian\nCaucasian\nCaucasian\nCaucasian\nCaucasian\nCaucasian\nCaucasian\n\n\n5\n0\nbinary 0/1\n3317\nrace\nOther\n0.665782\n0.323308\n0.852459\n0.302013\n0.455696\n0.147541\n0.676692\n0.697987\n0.544304\n79\n298\n0.023817\n0.209549\n36\n90\n208\n43\n133\n244\n377\n7214\n0.352785\n0.092506\n0.602147\n0.920466\n1.115085\n1.048203\n0.629057\n1.417970\n0.618447\n1.113660\n0.980490\nCaucasian\nCaucasian\nCaucasian\nCaucasian\nCaucasian\nCaucasian\nCaucasian\nCaucasian\nCaucasian\nCaucasian\n\n\n6\n0\nbinary 0/1\n3317\nsex\nFemale\n0.653763\n0.608434\n0.678930\n0.242537\n0.487310\n0.321070\n0.391566\n0.757463\n0.512690\n591\n804\n0.178173\n0.423656\n288\n195\n609\n303\n498\n897\n1395\n7214\n0.356989\n0.216801\n0.904348\n0.806925\n1.336425\n0.734738\n0.990343\n1.055810\n0.967101\n1.004633\n1.130710\nMale\nMale\nMale\nMale\nMale\nMale\nMale\nMale\nMale\nMale\n\n\n7\n0\nbinary 0/1\n3317\nsex\nMale\n0.653721\n0.629132\n0.675799\n0.330100\n0.364637\n0.324201\n0.370868\n0.669900\n0.635363\n2726\n3093\n0.821827\n0.468465\n994\n1021\n2072\n1732\n2753\n3066\n5819\n7214\n0.473105\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\nMale\nMale\nMale\nMale\nMale\nMale\nMale\nMale\nMale\nMale\n\n\n8\n0\nbinary 0/1\n3317\nage_cat\n25 - 45\n0.647846\n0.626257\n0.666216\n0.323112\n0.385135\n0.333784\n0.373743\n0.676888\n0.614865\n1924\n2185\n0.580042\n0.468240\n741\n706\n1479\n1183\n1889\n2220\n4109\n7214\n0.459723\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n25 - 45\n25 - 45\n25 - 45\n25 - 45\n25 - 45\n25 - 45\n25 - 45\n25 - 45\n25 - 45\n25 - 45\n\n\n9\n0\nbinary 0/1\n3317\nage_cat\nGreater than 45\n0.704315\n0.427711\n0.832096\n0.241117\n0.459391\n0.167904\n0.572289\n0.758883\n0.540609\n394\n1182\n0.118782\n0.250000\n181\n285\n897\n213\n498\n1078\n1576\n7214\n0.315990\n0.204782\n0.533914\n0.879232\n1.192804\n0.746232\n0.503031\n1.531238\n0.682963\n1.248989\n1.121136\n25 - 45\n25 - 45\n25 - 45\n25 - 45\n25 - 45\n25 - 45\n25 - 45\n25 - 45\n25 - 45\n25 - 45\n\n\n10\n0\nbinary 0/1\n3317\nage_cat\nLess than 25\n0.617397\n0.739583\n0.458647\n0.424528\n0.360360\n0.541353\n0.260417\n0.575472\n0.639640\n999\n530\n0.301176\n0.653368\n360\n225\n305\n639\n864\n665\n1529\n7214\n0.565075\n0.519231\n1.395369\n1.040293\n0.935673\n1.313873\n1.621868\n0.696781\n1.180958\n0.688435\n0.850173\n25 - 45\n25 - 45\n25 - 45\n25 - 45\n25 - 45\n25 - 45\n25 - 45\n25 - 45\n25 - 45\n25 - 45\n\n\n\n\n\n\n\nAs before, we can now use this matrix for creating plots in order to visualize disparities between races. In the next plot, we are comparing our reference group to all other races, using the False Positive values (as these can be considered the most problematic in this application).\nThe size of the boxes indicates the group size, the color and the according scale indicate the disparity between the compared groups. The plot shows that compared to the Caucasian reference group, the False Positive rate for African-American defendants is nearly two times higher, indicating a clear sign of unfairness/bias.\n\n\nCode\naqp.plot_disparity(bdf, group_metric='fpr_disparity', attribute_name='race', significance_alpha=0.05)\n\n\n\n\n\n\n\n\n\nAs an intermediate result, the aequitas-library works well for calculating key bias and fairness metrics and includes helpful functions for intuitively plotting the results. As this first part of the tutorial was aiming on giving a first glance at the data, it was largely built on the aequitas documentation. In the next part, we are shifting the focus away from bias detection and more towards bias mitigation. The second part introcudes a Neural Network that is trained for predicting recidivism and later tuned in order to find the optimal number of units and dropout rate."
  },
  {
    "objectID": "data_science/ds/objects/index.html#data-download-1",
    "href": "data_science/ds/objects/index.html#data-download-1",
    "title": "Bias in AI: detection and mitigation",
    "section": "3.1 Data Download",
    "text": "3.1 Data Download\nFirst step, as usual, is to get our data:\n\ndf_compas_bias = pd.read_csv(\"https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv\")\ndf_compas_bias.head()\n\n\n\n\n\n\n\n\n\nid\nname\nfirst\nlast\ncompas_screening_date\nsex\ndob\nage\nage_cat\nrace\n...\nv_decile_score\nv_score_text\nv_screening_date\nin_custody\nout_custody\npriors_count.1\nstart\nend\nevent\ntwo_year_recid\n\n\n\n\n0\n1\nmiguel hernandez\nmiguel\nhernandez\n2013-08-14\nMale\n1947-04-18\n69\nGreater than 45\nOther\n...\n1\nLow\n2013-08-14\n2014-07-07\n2014-07-14\n0\n0\n327\n0\n0\n\n\n1\n3\nkevon dixon\nkevon\ndixon\n2013-01-27\nMale\n1982-01-22\n34\n25 - 45\nAfrican-American\n...\n1\nLow\n2013-01-27\n2013-01-26\n2013-02-05\n0\n9\n159\n1\n1\n\n\n2\n4\ned philo\ned\nphilo\n2013-04-14\nMale\n1991-05-14\n24\nLess than 25\nAfrican-American\n...\n3\nLow\n2013-04-14\n2013-06-16\n2013-06-16\n4\n0\n63\n0\n1\n\n\n3\n5\nmarcu brown\nmarcu\nbrown\n2013-01-13\nMale\n1993-01-21\n23\nLess than 25\nAfrican-American\n...\n6\nMedium\n2013-01-13\nNaN\nNaN\n1\n0\n1174\n0\n0\n\n\n4\n6\nbouthy pierrelouis\nbouthy\npierrelouis\n2013-03-26\nMale\n1973-01-22\n43\n25 - 45\nOther\n...\n1\nLow\n2013-03-26\nNaN\nNaN\n2\n0\n1102\n0\n0\n\n\n\n\n5 rows × 53 columns"
  },
  {
    "objectID": "data_science/ds/objects/index.html#data-preprocessing",
    "href": "data_science/ds/objects/index.html#data-preprocessing",
    "title": "Bias in AI: detection and mitigation",
    "section": "3.1 Data Preprocessing",
    "text": "3.1 Data Preprocessing\nTo further work with the data, we need to clean it up and only keep variables that we want to use for our analysis. The following code leaves us with only the relevant variables: We want to focus our tutorial on racial bias in the COMPAS dataset, thus we only keep the data for “African-American” and “Caucasian” individuals.Further, we map the age variables to numeric values, convert “sex” and “charge_degree” (F: Felony M: Misdemeanor) to binary variables. Last but not least, we drop the “start2 and”end” variables after calculating the duration and adding it as a new variable.\n\n\n#Drop columns that we don't need\ncolumns_to_drop = ['id', 'name', 'first', 'last', 'compas_screening_date', 'dob', 'age', 'c_jail_in', 'c_jail_out',\n                   'c_case_number', 'c_offense_date', 'c_arrest_date', 'c_charge_desc', 'days_b_screening_arrest',\n                   'decile_score', 'r_case_number', 'r_days_from_arrest', 'r_offense_date', 'c_days_from_compas',\n                   'r_charge_degree', 'r_charge_desc', 'r_jail_in', 'r_jail_out', 'priors_count.1', 'violent_recid',\n                   'is_violent_recid', 'vr_case_number', 'vr_charge_degree', 'vr_offense_date', 'vr_charge_desc',\n                   'type_of_assessment', 'decile_score.1', 'score_text', 'is_recid', 'v_type_of_assessment',\n                   'screening_date', 'v_decile_score', 'v_score_text', 'v_screening_date', 'in_custody', 'out_custody',\n                   'event']\n\ndf_compas_bias_w = df_compas_bias.drop(columns=[col for col in columns_to_drop if col in df_compas_bias.columns])\n\n### Keep only white and black individuals\nrace_map = {'African-American':0, 'Caucasian':1, 'Asian':2, 'Hispanic':3, 'Native American':4, 'Other':5\n}\n\ndf_compas_bias_w['race'] = df_compas_bias_w['race'].apply(lambda x: race_map[x])\ndf_compas_bias_w = df_compas_bias_w[(df_compas_bias_w.race == 0.) | (df_compas_bias_w.race == 1.)]\n\n# Map age categories to numeric values\nage_map = {'Less than 25': 0, '25 - 45': 1, 'Greater than 45': 2}\ndf_compas_bias_w['age_cat'] = df_compas_bias_w['age_cat'].map(age_map)\n\n# Convert sex to binary values (Male: 0, Female: 1)\nsex_map = {'Male': 0, 'Female': 1}\ndf_compas_bias_w['sex'] = df_compas_bias_w['sex'].map(sex_map)\n\n# Convert charge degree to binary values (F: 1, M: 0)\ncharge_degree_map = {'F': 1., 'M': 0.}\ndf_compas_bias_w['c_charge_degree'] = df_compas_bias_w['c_charge_degree'].map(charge_degree_map)\n\n# Calculate duration in days and drop the original 'end' and 'start' columns\ndf_compas_bias_w['duration'] = (df_compas_bias_w['end'] - df_compas_bias_w['start'])\n\n#erase start and end\ndf_compas_bias_w = df_compas_bias_w.drop(columns=['start', 'end'])\n\ndf_compas_bias_w.head()\n\n\n\n\n\n\n\n\n\nsex\nage_cat\nrace\njuv_fel_count\njuv_misd_count\njuv_other_count\npriors_count\nc_charge_degree\ntwo_year_recid\nduration\n\n\n\n\n1\n0\n1\n0\n0\n0\n0\n0\n1.0\n1\n150\n\n\n2\n0\n0\n0\n0\n0\n1\n4\n1.0\n1\n63\n\n\n3\n0\n0\n0\n0\n1\n0\n1\n1.0\n0\n1174\n\n\n6\n0\n1\n1\n0\n0\n0\n14\n1.0\n1\n35\n\n\n8\n1\n1\n1\n0\n0\n0\n0\n0.0\n0\n745\n\n\n\n\n\n\n\n\n\nAs a next step, we want to separate our independent variables (X) from our target variable (Y) , namely “two_year_recid” (a Binary variable indicating whether the defendant is rearrested at within two years). Also, we use the MinMaxScaler from scikit-learn to normalize our features and the fit_transform method to rescale the features. We then have a look at our data, to see, if everything worked out well.\n\n\n# Separate features (X) and target variable (y)\nX = df_compas_bias_w.drop(columns=['two_year_recid']).values\ny = df_compas_bias_w['two_year_recid'].values\n\n# Rescale the features for better performance\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX = scaler.fit_transform(X)\n\n# Display the first few rows of the modified DataFrame\ndf_compas_bias_w.head()\n\n\n\n\n\n\n\n\n\nsex\nage_cat\nrace\njuv_fel_count\njuv_misd_count\njuv_other_count\npriors_count\nc_charge_degree\ntwo_year_recid\nduration\n\n\n\n\n1\n0\n1\n0\n0\n0\n0\n0\n1.0\n1\n150\n\n\n2\n0\n0\n0\n0\n0\n1\n4\n1.0\n1\n63\n\n\n3\n0\n0\n0\n0\n1\n0\n1\n1.0\n0\n1174\n\n\n6\n0\n1\n1\n0\n0\n0\n14\n1.0\n1\n35\n\n\n8\n1\n1\n1\n0\n0\n0\n0\n0.0\n0\n745"
  },
  {
    "objectID": "data_science/ds/objects/index.html#preprocessing-mitigation-techniques",
    "href": "data_science/ds/objects/index.html#preprocessing-mitigation-techniques",
    "title": "Bias in AI: detection and mitigation",
    "section": "3.3 Preprocessing Mitigation Techniques",
    "text": "3.3 Preprocessing Mitigation Techniques\nIn the next steps, we will predict, whether an individual will recidivate - in other words, we’ll precict the target variable, “two_year_recid”. To do this, we firstly split our data in a train and a test set. Then, we fit a logistic regression and print the accuracy and some other metrics.\n\n\n### Split to train / test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n### Fit logistic regression and print accuracy\nclf = LogisticRegression(random_state=42).fit(X_train, y_train)\ny_pred = clf.predict(X_test)\n\nacc = np.sum(y_pred == y_test) / len(y_test)\nprint(\"Accuracy =\", np.round(acc, 3))\n\n# Precision\nprecision = precision_score(y_test, y_pred)\nprint(\"Precision =\", np.round(precision, 3))\n\n# Recall\nrecall = recall_score(y_test, y_pred)\nprint(\"Recall =\", np.round(recall, 3))\n\n# F1 Score\nf1 = f1_score(y_test, y_pred)\nprint(\"F1 Score =\", np.round(f1, 3))\n\n# Confusion Matrix\nconf_matrix = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix:\\n\", conf_matrix)\n\nAccuracy = 0.874\nPrecision = 0.851\nRecall = 0.89\nF1 Score = 0.87\nConfusion Matrix:\n [[838 136]\n [ 96 775]]"
  },
  {
    "objectID": "data_science/ds/objects/index.html#ratio-of-african-american-to-caucasians-on-the-original-data-vs.-in-the-precicted-outcomes",
    "href": "data_science/ds/objects/index.html#ratio-of-african-american-to-caucasians-on-the-original-data-vs.-in-the-precicted-outcomes",
    "title": "Bias in AI: detection and mitigation",
    "section": "Ratio of African-American to Caucasians on the original data vs. in the precicted outcomes",
    "text": "Ratio of African-American to Caucasians on the original data vs. in the precicted outcomes\nLet’s have look at the distributions in our data, which plays an important role when dealing with fairness measures - as we learned in the first part of this tutorial. The code calculates the ratio of African-Americans in the data - apparently, for every Caucasian prisoner, there are approximately 1.51 African-American prisoners. The model predicts that African-American prisoners are 2.15 times more likely to reoffend than Caucasian prisoners. This ratio being higher than the actual ratio of African-American to Caucasian prisoners, indicates a potential bias in the predictions towards expecting higher recidivism among African-American individuals.\n\n# Identifying African-American and Caucasian individuals in the test set\nafr_am = (X_test[:, 2] == 0)\nwhite = (X_test[:, 2] == 1)\n\n# Predictions for African-American and Caucasian individuals\npred_afr_am = y_pred[afr_am]\npred_white = y_pred[white]\n\n# Calculating ratios\nafr_am_to_whites_ratio = afr_am.sum() / white.sum()\npredicted_reoffense_afr_am_to_whites_ratio = pred_afr_am.sum() / pred_white.sum()\n\n# Printing the ratios\nprint(\"Black to white prisoners ratio:\", round(afr_am_to_whites_ratio, 2))\nprint(\"Predicted reoffense blacks to whites ratio:\", round(predicted_reoffense_afr_am_to_whites_ratio, 2))\n\nBlack to white prisoners ratio: 1.51\nPredicted reoffense blacks to whites ratio: 2.15\n\n\n\n3.3.1 Using Disparate Impact Repairing (preprocessing)\nDisparate Impact is a fairness metric used to assess the equality of outcomes between two distinct groups: an unprivileged group and a privileged group. It measures the ratio of the proportion of individuals receiving favorable outcomes in the unprivileged group to that in the privileged group.\nDisparate impact remover is a preprocessing technique that edits feature values to increase group fairness while preserving rank-ordering within groups\n\\[\n\\frac{\\Pr(Y=1|D=\\text{unprivileged})}{\\Pr(Y=1|D=\\text{privileged})}\n\\]\n\nTo use the remover, let’s first create two dataframes, one for the privileged group (“Caucasian”) and one for the unprivileged group (“African-American”). Next, we want to plot the distribution of outcomes for each of the two groups and visually see this disparity:\n\n\n\nCode\n# Filter the DataFrame into privileged and unprivileged groups\nunprivileged = df_compas_bias_w[df_compas_bias_w['race'] == 0]\nprivileged = df_compas_bias_w[df_compas_bias_w['race'] == 1]\n\n# Set up the matplotlib figure\nplt.figure(figsize=(10, 6))\n\n# Plot the distribution for the unprivileged group\nsns.distplot(unprivileged['two_year_recid'], hist=False, label='Unprivileged (African-American)')\n\n# Plot the distribution for the privileged group\nsns.distplot(privileged['two_year_recid'], hist=False, label='Privileged (Caucasian)')\n\n# Add title and labels\nplt.title('Distribution of Outcomes for Privileged and Unprivileged Groups')\nplt.xlabel('Outcome (two_year_recid)')\nplt.ylabel('Density')\n\n# Show the plot\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nNext, we are creating a StandardDataset object using the AIF360 library, which is designed for fairness-aware machine learningand can be then used with various algorithms and metrics provided by AIF360 to assess and mitigate bias in machine learning models.\n\n\nprotected = 'race' #We chose race as the protected attribute because we are interested in the disparity between African-Americans and Caucasians\n\n# Create a StandardDataset object\ndf_protected = Dataset(df_compas_bias_w, #The dataset\n             label_name='two_year_recid', #The label or target variable that we want to predict\n             favorable_classes=[0], #The class we want to consider favorable (0 means no recidivism)\n             protected_attribute_names=[protected], #The attribute we want to test for disparity\n             privileged_classes=[[1]], categorical_features=[],   # 0 -&gt; Black, 1 -&gt; White to match the order of our dataset\n             features_to_keep=['race', 'priors_count', 'duration']) #The features we want to keep in the dataset because they are relevant for our model\n\n\n\nscaler = MinMaxScaler(copy=False) #We create a scaler object\n\n# Splitting the dataset into test and train sets like we did before\ntest, train = df_protected.split([0.33], seed=42) #We use a seed to make sure we get the same split every time\n\n# Fitting the scaler on the training features and transforming the training features using the same scaler\ntrain.features = scaler.fit_transform(train.features) #We fit the scaler on the training features\n\n# Transforming the test features using the same scaler as the one fitted on the training features\ntest.features = scaler.transform(test.features) #We transform the test features using the same scaler\n\n# Getting the index of the protected attribute in the feature names list\nindex = train.feature_names.index(protected) #We get the index of the protected attribute in the feature names list\n\n\nThe “repair level” parameter quantifies the extent to which you want to adjust the dataset to correct for disparities between privileged and unprivileged groups.\nIn the follwoing code chunk, we calculate the disparate impact for both privileged (e.g., Caucasians) and unprivileged (e.g., African-Americans) groups, storing the results in a list for each repair level.\n\n\nnp.random.seed(42)\n\n\n# Initialize a list to store the disparate impact values\nDIs = []\n\n# Iterate over each level of repair\nfor level in tqdm(np.linspace(0., 1., 11)):\n    # Initialize DisparateImpactRemover with the current level\n    di = DisparateImpactRemover(repair_level=level) #We initialize the DisparateImpactRemover with the current level\n    # Fit and transform the training data\n    train_repd = di.fit_transform(train) #We fit and transform the training data\n    # Transform the test data (do not fit the test data)\n    test_repd = di.fit_transform(test) #We transform the test data (do not fit the test data)\n\n    # Remove the protected attribute before training the model\n    X_tr = np.delete(train_repd.features, index, axis=1) #We remove the protected attribute before training the model\n    X_te = np.delete(test_repd.features, index, axis=1) #We remove the protected attribute before training the model\n    y_tr = train_repd.labels.ravel() #We get the labels for the training set\n\n    # Initialize and fit the logistic regression model\n    lmod = LogisticRegression(class_weight='balanced', solver='liblinear') #We initialize and fit the logistic regression model with class_weight='balanced' to account for the imbalance in the dataset\n    lmod.fit(X_tr, y_tr) #We fit the model\n\n    # Predict on the test set and copy the results to a new dataset\n    test_repd_pred = test_repd.copy() #We predict on the test set and copy the results to a new dataset\n    test_repd_pred.labels = lmod.predict(X_te) #We predict on the test set and copy the results to a new dataset\n\n    # Calculate and store the disparate impact\n    p = [{protected: 1}] #We calculate and store the disparate impact for the privileged group (in our case Caucasians)\n    u = [{protected: 0}] #We calculate and store the disparate impact for the unprivileged group (in our case African-Americans)\n    cm = BinaryLabelDatasetMetric(test_repd_pred, privileged_groups=p, unprivileged_groups=u) #We calculate and store the disparate impact for the privileged and unprivileged groups\n    DIs.append(cm.disparate_impact()) #We calculate and store the disparate impact for the privileged and unprivileged groups\n\n  0%|          | 0/11 [00:00&lt;?, ?it/s]  9%|▉         | 1/11 [00:00&lt;00:03,  3.21it/s] 27%|██▋       | 3/11 [00:00&lt;00:01,  7.90it/s] 45%|████▌     | 5/11 [00:00&lt;00:00,  8.15it/s] 64%|██████▎   | 7/11 [00:00&lt;00:00, 10.57it/s] 82%|████████▏ | 9/11 [00:00&lt;00:00, 12.71it/s]100%|██████████| 11/11 [00:00&lt;00:00, 11.19it/s]\n\n\n\nThe graph we’ll create next illustrates the efficacy of the Disparate Impact Remover at different repair levels for reducing racial bias within the context of predicting two-year recidivism. As the repair level increases from 0 (no adjustment) to 1 (full adjustment), the disparate impact metric approaches 1.0, which signifies fair treatment between races; each blue dot is a data point that shows how the fairness of the model, as measured by the disparate impact, changes as we apply different levels of bias mitigation.\nInitially, with no repair, the metric is close to 0.8, indicating substantial bias against the unprivileged group (likely African-Americans). With full repair, the bias is reduced, but not entirely eliminated, suggesting some residual unfairness remains or that the repair cannot fully compensate for the existing disparities within the data. The graph underscores the challenge of achieving complete fairness, as represented by the elusive ideal disparate impact value of 1.0, even as efforts are made to correct for bias in predictive modeling for criminal justice.\n\n\n\nCode\n# Assuming DIs is your list of disparate impact values\nrepair_levels = np.linspace(0., 1., 11)\n\n# Assuming DIs is defined before this point\nDIs = DIs[:11]\n\nplt.figure(figsize=(10, 6))\nplt.plot(repair_levels, DIs, marker='o', markersize=8, linewidth=2, label='Disparate Impact')\n\nplt.title('Disparate Impact vs. Repair Level', fontsize=14)\nplt.xlabel('Repair Level', fontsize=12)\nplt.ylabel('Disparate Impact', fontsize=12)\n\nplt.axhline(y=1.0, color='g', linestyle='--', label='Ideal Value')\nplt.plot([0, 1], [0.8, 0.8], 'r', linestyle='-.', label='Threshold')\n\nplt.grid(True)\nplt.legend()\nplt.tight_layout()\n\n# Display the plot\nplt.show()\n\n\n\n\n\nCode\n# Predict on test using only the features at indices -3 and -1\ny_pred = lmod.predict(X_test[:, [-3, -1]])\n\n# Boolean masks for African-American and Caucasian individuals\nafr_am_mask = (X_test[:, 2] == 0)\nwhite_mask = (X_test[:, 2] == 1)\n\n# Predictions for African-American and Caucasian individuals\npred_afr_am = y_pred[afr_am_mask]\npred_white = y_pred[white_mask]\n\n# Calculating the ratio of predicted reoffenses\npredicted_reoffense_afr_am_to_whites_ratio = pred_afr_am.sum() / pred_white.sum()\n\n# Printing the predicted reoffense ratio\nprint(\"Predicted reoffense blacks to whites ratio:\", np.round(predicted_reoffense_afr_am_to_whites_ratio, 2))\n\n\nPredicted reoffense blacks to whites ratio: 2.07\n\n\n\nAnd good news: The Ratio dropped from 2.15 to 2.07! So, reparation has worked, we have a less biased result. But, let’s be honest, this is not a huge improvement, there still seems to be a decent amount of bias. So, let’s try out some other techniques to improve the fairness of the model even further!"
  },
  {
    "objectID": "data_science/ds/objects/index.html#reweighting-the-data-preprocessing",
    "href": "data_science/ds/objects/index.html#reweighting-the-data-preprocessing",
    "title": "Bias in AI: detection and mitigation",
    "section": "3.3.2 Reweighting the Data (Preprocessing)",
    "text": "3.3.2 Reweighting the Data (Preprocessing)\nFirst, let’s have a look at the difference in mean outcomes metric of the AIF360 library for unprivileged and privileged groups. A positive value indicates a bias in favor of the privileged group, while a negative value indicates a bias against the unprivileged group. In our case, the latter is the case, with a difference in mean outcomes of -0.126\n\n\nCode\nfrom aif360.metrics import BinaryLabelDatasetMetric\n\n# Define unprivileged and privileged groups.\n\nprivileged_groups = [{'race': 1}]\nunprivileged_groups = [{'race': 0}]\n\n\n# Initialize the metric for the original training dataset\nmetric_orig_train = BinaryLabelDatasetMetric(train, # train from protected dataset\n                                             unprivileged_groups=unprivileged_groups, # Consideres to be at disadvantage (e.g., Blacks)\n                                             privileged_groups=privileged_groups) # Considered to be at an advantage (e.g., Caucasians)\n\n# Print the difference in mean outcomes between unprivileged and privileged groups\nmean_diff = metric_orig_train.mean_difference() # difference in probabilities of favorable outcomes between the privileged and unprivileged groups.\n#A positive value indicates a bias in favor of the privileged group, while a negative value indicates a bias against the unprivileged group.\nprint(\"Difference in mean outcomes between blacks and whites = %f\" % mean_diff)\n\n\nDifference in mean outcomes between blacks and whites = -0.125928\n\n\n\nNext, we apply a second mitigation strategy by performing reweighing on the training dataset to ensure that the total sum of instance weights for the transformed training dataset (transf_train.instance_weights.sum()) is approximately equal to the total sum of instance weights for the original training dataset (train.instance_weights.sum()). The check is done using an absolute difference and comparing it against a small number, 1e-6, to account for any minor floating-point arithmetic discrepancies.\nThe result “True” indicates that the reweighing algorithm is functioning as intended in this regard: it’s modifying individual instance weights to address disparities without changing the overall weight sum. This is important to ensure that the dataset’s overall statistical properties remain consistent while individual instances are weighted differently to mitigate bias.\n\nRW = Reweighing(unprivileged_groups=unprivileged_groups,\n               privileged_groups=privileged_groups)\nRW.fit(train)\ntransf_train = RW.transform(train)\n\n\n### Testing\nnp.abs(transf_train.instance_weights.sum()-train.instance_weights.sum())&lt;1e-6\n\nTrue\n\n\n\n\n\nCode\nmetric_orig_train = BinaryLabelDatasetMetric(train,\n                                             unprivileged_groups=unprivileged_groups,\n                                             privileged_groups=privileged_groups)\norig_mean_difference = metric_orig_train.mean_difference()\n\n\nmetric_transf_train = BinaryLabelDatasetMetric(transf_train,\n                                               unprivileged_groups=unprivileged_groups,\n                                               privileged_groups=privileged_groups)\ntransf_mean_difference = metric_transf_train.mean_difference()\n\nprint(\"Difference in mean outcomes between transformed blacks and whites = %f\" % metric_transf_train.mean_difference())\n\n\nDifference in mean outcomes between transformed blacks and whites = -0.000000\n\n\n\nThe following plot shows that the transformation of the data does not change the overall statistic properties - the distribution of features stays the same after the transformation.\n\n\n\nCode\noriginal_labels = train.labels.ravel()\ntransformed_labels = transf_train.labels.ravel()\n\noriginal_feature = train.features[:, train.feature_names.index('priors_count')]\ntransformed_feature = transf_train.features[:, transf_train.feature_names.index('priors_count')]\n\n# Plotting distributions\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\n# Plot distribution of labels\nsns.histplot(original_labels, ax=axes[0, 0], color=\"blue\", kde=True, stat=\"density\", linewidth=0)\naxes[0, 0].set_title('Original Labels')\nsns.histplot(transformed_labels, ax=axes[0, 1], color=\"green\", kde=True, stat=\"density\", linewidth=0)\naxes[0, 1].set_title('Transformed Labels')\n\n# Plot distribution of a key feature\nsns.histplot(original_feature, ax=axes[1, 0], color=\"blue\", kde=True, stat=\"density\", linewidth=0)\naxes[1, 0].set_title('Original Feature Distribution')\nsns.histplot(transformed_feature, ax=axes[1, 1], color=\"green\", kde=True, stat=\"density\", linewidth=0)\naxes[1, 1].set_title('Transformed Feature Distribution')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "data_science/ds/objects/index.html#train-logistic-regression-with-reweighed-dataset",
    "href": "data_science/ds/objects/index.html#train-logistic-regression-with-reweighed-dataset",
    "title": "Bias in AI: detection and mitigation",
    "section": "Train Logistic Regression with reweighed dataset",
    "text": "Train Logistic Regression with reweighed dataset\nNow, let’s see, if we can train a fairer model, using this preprocessing technique of reweighting - promising to make our results less bias. First, we neet to train with the tranformed data and then predict our outcome on the transformed test dataset.\n\nscale_transf = StandardScaler()\nX_reweighed_train = scale_transf.fit_transform(transf_train.features)\ny_reweighed_train = transf_train.labels.ravel()\n\nlmod = LogisticRegression()\nlmod.fit(X_reweighed_train, y_reweighed_train,\n        sample_weight=transf_train.instance_weights)\ny_train_pred = lmod.predict(X_reweighed_train)\n\ntransf_test = RW.transform(test)\n\nscale_transf = StandardScaler()\nX_reweighed_test = scale_transf.fit_transform(transf_test.features)\ny_reweighed_test = transf_test.labels.ravel()\n\n# Predict on test\ny_pred = lmod.predict(X_reweighed_test)\nX_test\n\narray([[0.        , 0.        , 0.        , ..., 0.02631579, 1.        ,\n        0.40499307],\n       [0.        , 0.5       , 1.        , ..., 0.        , 1.        ,\n        0.82385576],\n       [0.        , 1.        , 0.        , ..., 0.26315789, 1.        ,\n        0.24479889],\n       ...,\n       [0.        , 0.5       , 0.        , ..., 0.05263158, 1.        ,\n        0.18862691],\n       [0.        , 0.        , 0.        , ..., 0.        , 1.        ,\n        0.93273232],\n       [1.        , 0.5       , 0.        , ..., 0.42105263, 1.        ,\n        0.60679612]])\n\n\n\nWe want to print the calculated ratios, providing insights into the relative proportions of African-American to Caucasian individuals and the predicted reoffense ratios based on the model’s predictions on the transformed data, as we did for the original data in the beginning. And wohoo! The predictions are now closer to the ratio, meaning that our prediction got less biased. And: we didn’t have to sacrifice accuracy for that!\n\n\n\nCode\n# Create boolean masks for African-American and Caucasian groups\n# Make sure these masks are created from the dataset used for the predictions\nafr_am_mask = (transf_test.features[:, 0] == 0)\nwhite_mask = (transf_test.features[:, 0] == 1)\n\n# Apply the masks to the predictions\npred_afr_am = y_pred[afr_am_mask]\npred_white = y_pred[white_mask]\n\n# Calculate the Black to White Prisoners Ratio\n# Count of African-American individuals divided by count of Caucasian individuals\nratio_afr_am_white = np.sum(afr_am_mask) / np.sum(white_mask)\n\n# Calculate the Predicted Reoffense Ratio for Blacks to Whites\n# Sum of predicted reoffenses for African-Americans divided by sum for Caucasians\nratio_pred_reoffense = np.sum(pred_afr_am == 1) / np.sum(pred_white == 1)\n\nprint(\"Black to white prisoners ratio:\", np.round(ratio_afr_am_white, 2))\nprint(\"Predicted reoffense blacks to whites ratio:\", np.round(ratio_pred_reoffense, 2))\n\n\nBlack to white prisoners ratio: 1.5\nPredicted reoffense blacks to whites ratio: 1.85\n\n\n\n\nCode\ny_pred = lmod.predict(X_reweighed_test)\n\nacc = np.sum(y_pred == y_reweighed_test) / len(y_reweighed_test)\nprint(\"Accuracy =\", np.round(acc, 3))\n\n\nAccuracy = 0.875\n\n\n\nprivileged_groups_new = [{protected: 1}]\nunprivileged_groups_new = [{protected: 0}]\n\n\ncm = BinaryLabelDatasetMetric(transf_test.copy(), #\n                              privileged_groups=privileged_groups_new,\n                              unprivileged_groups=unprivileged_groups_new)\n\nDIs.append(cm.disparate_impact())\n\n\nWhen we apply the reweighted dataset to train a model and subsequently calculate fairness metrics like Disparate Impact (DI), we typically expect the DI values to move towards 1, signaling reduced bias. The marked increase in DI at this full repair level indicates that the algorithm has substantially adjusted the instance weights, aiming to correct for disparities.\n\n\n\nCode\n# Assuming DIs is your list of disparate impact values with the appropriate length\nplt.figure(figsize=(8, 5))\n\n# Plot with increased marker size and line width\nplt.plot(np.linspace(0, 1, 12), DIs, marker='o', markersize=8, linewidth=2, label='Disparate Impact')\n\n# Green line for the ideal value of DI = 1\nplt.plot([0, 1], [1, 1], 'g', linestyle='--', label='Ideal Value')\n\n# Red line for threshold\nplt.plot([0, 1], [0.8, 0.8], 'r', linestyle='-.', label='Threshold')\n\nplt.ylim([0.7, 1.1])\n\n# Setting font size for labels and title\nplt.ylabel('Disparate Impact (DI)', fontsize=16)\nplt.xlabel('Repair Level', fontsize=16)\n\n# Adding grid, legend, and tight layout\nplt.grid(True)\nplt.legend()\nplt.tight_layout()\n\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "data_science/ds/objects/index.html#post-processing-mitigation-strategy",
    "href": "data_science/ds/objects/index.html#post-processing-mitigation-strategy",
    "title": "Bias in AI: detection and mitigation",
    "section": "Post-processing Mitigation Strategy",
    "text": "Post-processing Mitigation Strategy\n\nReject Option Classification\nLast but not least we want to present a post-processing strategy to mitigate bias. The goal of Reject Option Classification is to improve the fairness of predictions made by a classifier. Let’s give you an overview of this technique:\n\nClassifier Prediction: First, you have a classifier that has been trained on your dataset and makes predictions about new instances.\nConfidence Interval: Reject Option Classification operates on a confidence interval around the decision boundary of the classifier. This is where the classifier’s certainty about its predictions is lower.\nProtected Attribute: It uses a protected attribute (in our case: race) to determine where bias might be present in these uncertain predictions.\nFavorable Outcomes: For instances within the confidence interval, if they belong to the unprivileged group, the algorithm can change an unfavorable outcome to a favorable one. Conversely, for the privileged group, it can change a favorable outcome to an unfavorable one.\nFairness Enhancement: The idea is to “reject” the initial decision (hence the name) in favor of one that will lead to a more balanced distribution of positive outcomes between the privileged and unprivileged groups.\nBalancing Performance and Fairness: This method aims to balance the overall performance of the classifier (in terms of accuracy, precision, etc.) with fairness considerations. It seeks to ensure that the positive predictive value (the probability that subjects with a positive screening test truly have the disease) is similar across groups.\n\n\n#Identifying the Favorable Outcome Index\npos_ind = np.where(lmod.classes_ == transf_train.favorable_label)[0][0] #We identify the favorable outcome index\n\n#Fit the scaler on the training data and transform the training features\nscale_transf = StandardScaler() #We fit the scaler on the training data and transform the training features\nX_reweighed_train = scale_transf.fit_transform(transf_train.features) #We fit the scaler on the training data and transform the training features\n\n#Transform the test features using the same scaler\nX_reweighed_test = scale_transf.transform(transf_test.features) #We transform the test features using the same scaler\n\n#Copy the datasets\ntransf_train_post = transf_train.copy() #We copy the datasets\ntransf_test_post = transf_test.copy() #We copy the datasets\n\n#Predict probabilities and assign scores for the training and test sets\ntransf_train_post.scores = lmod.predict_proba(X_reweighed_train)[:, pos_ind].reshape(-1, 1) #We predict probabilities and assign scores for the training and test sets\ntransf_test_post.scores = lmod.predict_proba(X_reweighed_test)[:, pos_ind].reshape(-1, 1) #We predict probabilities and assign scores for the training and test sets\n\n\nmetric_ub = 0.05 #We set the upper bound for the metric\nmetric_lb = -0.05 #We set the lower bound for the metric\n\nROC = RejectOptionClassification(unprivileged_groups=unprivileged_groups, #We set the upper bound for the metric\n                                 privileged_groups=privileged_groups, #We set the lower bound for the metric\n                                 low_class_thresh=0.01, #Range of decision thresholds to consider for potential adjustment.\n                                 high_class_thresh=0.99, #Range of decision thresholds to consider for potential adjustment.\n                                  num_class_thresh=100, #Number of decision thresholds to be considered within the specified range\n                                  num_ROC_margin=50, #Number of margins to be examined around the decision threshold where ROC will adjust the classifier's predictions.\n                                  metric_name= 'Statistical parity difference', #Metric used to measure fairness for determining thresholds\n                                  metric_ub=metric_ub, metric_lb=metric_lb) #Upper and lower bound for the fairness metric used to determine thresholds\n\nROC = ROC.fit(transf_test, transf_test_post)\n\nThe technique learns how to adjust the decision thresholds based on the scores and labels in transf_test_post such that the fairness metric (Statistical parity difference in this case) falls between the bounds specified earlier (metric_ub and metric_lb). The dataset transf_test is used as a reference for the original decision making, which transf_test_post presumably improves upon.\n\n\nCode\nprint(\"Optimal classification threshold (with fairness constraints) = %.4f\" % ROC.classification_threshold)\nprint(\"Optimal ROC margin = %.4f\" % ROC.ROC_margin)\n\n\nOptimal classification threshold (with fairness constraints) = 0.8217\nOptimal ROC margin = 0.0146\n\n\nROC algorithm has identified 0.8217 as the decision threshold that best balances the classifier’s performance with the fairness constraints that were set. Optimal ROC margin = 0.0146. This margin is where the algorithm is most active in adjusting predictions to improve fairness. if an instance belonging to the unprivileged group has a score slightly below the threshold, it might be pushed above the threshold to receive a favorable outcome. Conversely, an instance from the privileged group with a score just above the threshold might be pulled below it to receive an unfavorable outcome. the Algorithm has found a way to adjust the classifier’s predictions to conform to the fairness constraints without excessively compromising its predictive performance\nThe purpose of this function is to evaluate the performance of a classification model from both a standard accuracy perspective and a fairness perspective.\n\ndef compute_metrics(dataset_true, dataset_pred,\n                    unprivileged_groups, privileged_groups,\n                    disp=True):\n    \"\"\" Compute the key metrics \"\"\"\n    classified_metric_pred = ClassificationMetric(dataset_true, dataset_pred,\n                                                  unprivileged_groups=unprivileged_groups,\n                                                  privileged_groups=privileged_groups)\n    # Initialize an empty OrderedDict\n    # Compute all required metrics at once\n    metrics = OrderedDict([\n        (\"Balanced accuracy\", 0.5 * (classified_metric_pred.true_positive_rate() +\n                                     classified_metric_pred.true_negative_rate())),\n        (\"Statistical parity difference\", classified_metric_pred.statistical_parity_difference()),\n        (\"Disparate impact\", classified_metric_pred.disparate_impact()),\n        (\"Average odds difference\", classified_metric_pred.average_odds_difference()),\n        (\"Equal opportunity difference\", classified_metric_pred.equal_opportunity_difference()),\n        (\"Theil index\", classified_metric_pred.theil_index())\n    ])\n\n    if disp:\n        # Display metrics in a formatted way\n        for metric_name, metric_value in metrics.items():\n            print(f\"{metric_name} = {metric_value:.4f}\")\n\n    return metrics\n\n\n# Metrics for the transformed test set\ndataset_transf_test_post = ROC.predict(transf_test_post)\n\nmetric_test_aft = compute_metrics(transf_test, dataset_transf_test_post,\n                unprivileged_groups, privileged_groups)\n\nBalanced accuracy = 0.8963\nStatistical parity difference = -0.0066\nDisparate impact = 0.9852\nAverage odds difference = -0.0173\nEqual opportunity difference = -0.0381\nTheil index = 0.1148\n\n\nUsing this technique, we get very nice results for both: Our model’s performance and its fairness measures.\nBalanced Accuracy (0.8963) :\nGood overall accuracy of the model. Balanced accuracy takes into account both the true positive rate and true negative rate.\nStatistical Parity Difference (-0.0066)\nIt indicates that the probability of a positive outcome (favorable prediction) is nearly equal for both privileged and unprivileged groups\nDisparate Impact (0.9852)\nA value close to 1 indicates fair treatment between the groups. Specifically, a value of 1 would imply perfect fairness. The model’s predictions do not disproportionately favor one group over the other\nAverage Odds Difference (-0.0173)\nEquality of odds between unprivileged and privileged groups. A value of 0 would mean perfect equality. The value -0.0173 indicates a small bias against the unprivileged group in terms of false positive and true positive rates, but this bias is relatively minor now.\nEqual Opportunity Difference (-0.0381)\nTrue positive rates between groups. A value of 0 represents equal opportunity. The value -0.0381 suggests a slight bias against the unprivileged group in terms of having true positives or favorable outcomes.\nTheil Index (0.1148)\nTheil index is a measure of inequality. A value of 0 indicates perfect equality, while higher values show greater inequality. A value of 0.1148 indicates some inequality in the model’s predictions, but it’s not excessively high"
  },
  {
    "objectID": "data_science/ds/objects/index.html#results-and-discussion",
    "href": "data_science/ds/objects/index.html#results-and-discussion",
    "title": "Bias in AI: detection and mitigation",
    "section": "Results and Discussion",
    "text": "Results and Discussion\nThe tutorial introduced how to detect bias and helps the user to replicate a biased CNN in order to understand that bias is something, you always have to keep in mind when training your model.\nIn the last and most central part of the tutorial we introduced different approaches to mitigate bias. To conclude, we can state that we successfully debiased the data, as the last list of performance and fairness metric demonstrates. Beginning with relatively high bias, we applied - first, we used the disparate impact remover, showing some, but not an exceptional improvement in bias metrics. - We could reach a better result, applying our second preprocessing strategy, namely reweighting our data before training our model with it. - Last but not least, we applied a post processing technique, Reject Option Classification, that also lead to good results, debiasing our data.\nAll of these steps demonstrate that there are different strategies for each part of our data and model pipeline to adress bias and find strategies to come to fairer results."
  },
  {
    "objectID": "data_science/ds/objects/index.html#limitations",
    "href": "data_science/ds/objects/index.html#limitations",
    "title": "Bias in AI: detection and mitigation",
    "section": "Limitations",
    "text": "Limitations\nThe greatest limitation of this tutorial potentially comes from the chosen data source: The COMPAS data already comes in a quite tidy format and previous analyses already identified the most pressing issues of the data set. When applying the detection and mitigation techniques demonstrated in this tutorial, students and researcher should be aware that alternative data sets need to be carefully cleanedbefore calculating bias metrices and applying mitigation strategies.\nMoreover, this tutorial focuses on one specific data input type. When other data types are considered (e.g. image, video, or time-series), detection and mitigation strategies potentially need to be adapted.\nAs introduced in the tutorial memo, there are numerous distinct types of biases that can occur in the machine/deep learning pipeline. This tutorial focused on biased data, especially on representation bias. Applied to different data sets, students and researchers should also check for other types of bias (e.g. measurement bias). Additionally, not only the data should be tested for bias, but also implemented models and model evaluating should be critically exmined."
  },
  {
    "objectID": "data_science/ds/objects/index.html#next-steps",
    "href": "data_science/ds/objects/index.html#next-steps",
    "title": "Bias in AI: detection and mitigation",
    "section": "Next Steps",
    "text": "Next Steps\nThis tutorial serves as a first introduction to the topic of bias and fairness - we hope that it gave a good overview of the related issues and possible solutions to tackle them. As next steps we propose that users of the tutorial look for another dataset and try to apply the newly acquired tools (unfortuately it is not a challenge to find biased datasets on Kaggle or other online platforms).\nTo dig deeper in the deep learning application it could be interesting to check out the topic of bias and fairness in other data types like images and text data. It would have exceeded the scope of our tutorial but there are very interesting approaches of bias mitigation in Natural Language Processing, for example the approach of text pertubation to train fairer models (https://ai.meta.com/blog/measure-fairness-and-mitigate-ai-bias/). For policy application, text data is very important thus we can only recommend the users to get familiar with such techniques.\nTo conclude, we know that we could only show a very small part of bias detection and mitigation strategies - so we don’t expect the users to be experts for this area coming out of this tutorial. More importantly we hope that we might have shifted the user’s view on the importance of bias related issues. This could be a first step to raise awareness for his crucial part of machine and deep learning - which so far is too often ommitted or neglected."
  },
  {
    "objectID": "about/about_us.html#hi",
    "href": "about/about_us.html#hi",
    "title": "{{< fa solid scale-balanced >}} Ethics <span style='font-size: 18px;'>&#x2715;</span> Data Science {{< fa solid code-branch >}}",
    "section": "Hi",
    "text": "Hi"
  },
  {
    "objectID": "about/about_us.html",
    "href": "about/about_us.html",
    "title": "Ethics ✕ Data Science",
    "section": "",
    "text": "Ethics ✕ Data Science aims to improve graduate education in international affairs at the intersection of data science and ethics. It rests on the belief that to train the next generation of data science practitioners and researchers, we need to develop ways of teaching approaches that integrate data science and data ethics.\nThe key challenge for graduate programs is how to teach applied ethics for non-ethicists and data science for non-data-scientists.\nData Science has become a key component of social science curricula. As data scientists, we teach our students how to use various data to model, predict and understand human behavior. The tools that we put in our students’ hands are analytically powerful but ethically ambivalent. Despite the potential risks to research subjects and society, we tend to neglect the intellectual and social skills that would enable our students to lead difficult conversations about the values that should guide data collection, analysis, and management.\nData Science has also become a major topic in philosophy and applied ethics. As philosophers, we teach our students about the ethical challenges of the digital age, the permissibility of automated decision making, the limits of digital rights, the meaning of privacy, the demands of responsible scientific practices, and much more. However, we tend to neglect the technical fundamentals necessary to understand and evaluate decisions in the data science workflow.\nEthics ✕ Data Science seeks to provide materials for both areas, teaching scenarios and case studies.\n\n\nEthics ✕ Data Science is a collaboration between Johannes Himmelreich (Maxwell School at Syracuse University) and Simon Munzert (Hertie School).\n\n\n\nThe project was funded by APSIA, the Association of Professional Schools of International Affairs, with a grant from the a APSIA Faculty Fund. The Hertie School supported a two-day workshop in Berlin in 2021."
  },
  {
    "objectID": "about/about_us.html#members",
    "href": "about/about_us.html#members",
    "title": "Ethics ✕ Data Science",
    "section": "",
    "text": "Ethics ✕ Data Science is a collaboration between Johannes Himmelreich (Maxwell School at Syracuse University) and Simon Munzert (Hertie School)."
  },
  {
    "objectID": "about/about_us.html#funding-acknowledgement",
    "href": "about/about_us.html#funding-acknowledgement",
    "title": "Ethics ✕ Data Science",
    "section": "",
    "text": "The project was funded by APSIA, the Association of Professional Schools of International Affairs, with a grant from the a APSIA Faculty Fund. The Hertie School supported a two-day workshop in Berlin in 2021."
  },
  {
    "objectID": "ethics/ethics/privacy-considerations/index.html",
    "href": "ethics/ethics/privacy-considerations/index.html",
    "title": "The Pathology of Privacy",
    "section": "",
    "text": "Main idea\n\n\n\nProtecting privacy is a collective effort. Individuals can only do so much to protect personal information. This undermines the importance of consent and makes it rational to not know the many ways in which your privacy is violated."
  },
  {
    "objectID": "ethics/ethics/privacy-considerations/index.html#what-do-you-know",
    "href": "ethics/ethics/privacy-considerations/index.html#what-do-you-know",
    "title": "The Pathology of Privacy",
    "section": "What do you know?",
    "text": "What do you know?\nSometimes, however, you do know how limited your privacy is. You remember there were privacy settings in that app that you just couldn’t bother to check. And you know that online advertisers track you. And you know that you signed something at your doctor’s office about sharing your patient data.\nBut often, privacy today is violated unwittingly. For one, it’s basically impossible to be aware of all the ways in which your data is collected, analyzed, and shared. Similarly, nobody can possibly read all the terms and conditions and privacy policies that they are agreeing to.\nIn short, there is an information problem: Individuals don’t know — and, to some extent, cannot know — all they need to know to safeguard their privacy. And, arguably, we shouldn’t spend much of our time and mental energy on protecting our privacy."
  },
  {
    "objectID": "ethics/ethics/privacy-considerations/index.html#what-can-you-do",
    "href": "ethics/ethics/privacy-considerations/index.html#what-can-you-do",
    "title": "The Pathology of Privacy",
    "section": "What can you do?",
    "text": "What can you do?\nThe second part of the state of individual privacy today has to do with choices. We cannot throw away all of our devices, like Ron Swanson does. The options that we have to protect our privacy, even when we are aware of them, is practically limited. Metadata collection that is required by law is hard to opt out of. Even tech-savvy individuals are outgunned by the professional data, surveillance, and security industry.\nAnd then, we also have a cognitive system that is easy to exploit. As such, websites and apps use various forms of deception to get you to do things.1 You end up subscribing to a service without knowing, agreeing to terms without having read them, or clicking “Continue” because the other button, “No,” was made hard to see.\n\n\n\n\n\nDeceptive design\n\n\nFinally, much information is not unique to individuals. This means that protecting that information is not up to you. How much you can do to protect your privacy has in-principle limits when so many others have — and can share — the same information that you consider personal to you. Because much information is shared across a large number of people, no individual can control how their information is shared. This is sometimes called the externality of data.\nOne clear example of this is genetic information. Because most genetic information is shared and not unique to one individual, you can’t protect most of your genetic information. You can’t stop you aunt or brother from using a genetic testing service. Still, you would consider your genetic information, even parts of it, to be your personal information.\nAnother example TBC"
  },
  {
    "objectID": "ethics/ethics/privacy-considerations/index.html#footnotes",
    "href": "ethics/ethics/privacy-considerations/index.html#footnotes",
    "title": "The Pathology of Privacy",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee https://www.deceptive.design↩︎"
  },
  {
    "objectID": "ethics/ethics/privacy-considerations/index.html#privacy-as-a-collective-property",
    "href": "ethics/ethics/privacy-considerations/index.html#privacy-as-a-collective-property",
    "title": "The Pathology of Privacy",
    "section": "Privacy as a collective property",
    "text": "Privacy as a collective property\nTBC"
  },
  {
    "objectID": "ethics/ethics/privacy-considerations/index.html#against-consent",
    "href": "ethics/ethics/privacy-considerations/index.html#against-consent",
    "title": "The Pathology of Privacy",
    "section": "Against Consent",
    "text": "Against Consent\nAll of this is bad news for consent."
  },
  {
    "objectID": "ethics/ethics/pathology-of-privacy/index.html",
    "href": "ethics/ethics/pathology-of-privacy/index.html",
    "title": "The Pathology of Privacy",
    "section": "",
    "text": "Main idea\n\n\n\nProtecting privacy is a collective effort. Individuals can only do so much to protect personal information. This undermines the importance of consent and makes it rational to not know the many ways in which your privacy is violated."
  },
  {
    "objectID": "ethics/ethics/pathology-of-privacy/index.html#what-do-you-know",
    "href": "ethics/ethics/pathology-of-privacy/index.html#what-do-you-know",
    "title": "The Pathology of Privacy",
    "section": "What do you know?",
    "text": "What do you know?\nSometimes, however, you do know how limited your privacy is. You remember there were privacy settings in that app that you just couldn’t bother to check. And you know that online advertisers track you. And you know that you signed something at your doctor’s office about sharing your patient data.\nBut often, privacy today is violated unwittingly. For one, it’s basically impossible to be aware of all the ways in which your data is collected, analyzed, and shared. Similarly, nobody can possibly read all the terms and conditions and privacy policies that they are agreeing to.\nIn short, there is an information problem: Individuals don’t know — and, to some extent, cannot know — all they need to know to safeguard their privacy. And, arguably, we shouldn’t spend much of our time and mental energy on protecting our privacy."
  },
  {
    "objectID": "ethics/ethics/pathology-of-privacy/index.html#what-can-you-do",
    "href": "ethics/ethics/pathology-of-privacy/index.html#what-can-you-do",
    "title": "The Pathology of Privacy",
    "section": "What can you do?",
    "text": "What can you do?\nThe second part of the state of individual privacy today has to do with choices. We cannot throw away all of our devices, like Ron Swanson does. The options that we have to protect our privacy, even when we are aware of them, is practically limited. Metadata collection that is required by law is hard to opt out of. Even tech-savvy individuals are outgunned by the professional data, surveillance, and security industry.\nAnd then, we also have a cognitive system that is easy to exploit. As such, websites and apps use various forms of deception to get you to do things.1 You end up subscribing to a service without knowing, agreeing to terms without having read them, or clicking “Continue” because the other button, “No,” was made hard to see.\n\n\n\n\n\nDeceptive design\n\n\nFinally, much information is not unique to individuals. This means that protecting that information is not up to you. How much you can do to protect your privacy has in-principle limits when so many others have — and can share — the same information that you consider personal to you. Because much information is shared across a large number of people, no individual can control how their information is shared. This is sometimes called the externality of data.\nOne clear example of this is genetic information. Because most genetic information is shared and not unique to one individual, you can’t protect most of your genetic information. You can’t stop you aunt or brother from using a genetic testing service. Still, you would consider your genetic information, even parts of it, to be your personal information.\nAnother example is tastes in music, books, movies, or fashion. Conditional on features such as age, income, gender, race, nationality, and job, peoples’ tastes in music, books, or fashion are somewhat similar. This means that even if you don’t share with Amazon what music, books, movies, or clothes you like, if someone does, who is enough like you, Amazon can infer this information about you."
  },
  {
    "objectID": "ethics/ethics/pathology-of-privacy/index.html#privacy-as-a-collective-property",
    "href": "ethics/ethics/pathology-of-privacy/index.html#privacy-as-a-collective-property",
    "title": "The Pathology of Privacy",
    "section": "Privacy as a collective property",
    "text": "Privacy as a collective property\nPrivacy can still be protected — just not by individuals on their own. The protection of privacy is a collective action problem. Collectively, with the right norms or laws and their enforcement, we can protect our privacy. It is only that, as individuals there is only so much that each of us can do.\nTogether, however, we could make terms and conditions more accessible and informative, we could foster competition and alternatives to services and products where currently we find it hard to opt-out (interoperability can counter the natural monopoly tendency of social networks, for example).\nIn fact, such a collective approach to safeguarding privacy is necessary because of the externality of data. Norms are required to restrict how others share information because their doing so affects me — and might harm me. My great cousin and I own shared information, i.e., our genetic data. Insofar as it is a potential harm to me if this data were passed on to a third party, harm prevention could be one reason to restrict how and with whom my great cousin can share our shared information."
  },
  {
    "objectID": "ethics/ethics/pathology-of-privacy/index.html#against-consent",
    "href": "ethics/ethics/pathology-of-privacy/index.html#against-consent",
    "title": "The Pathology of Privacy",
    "section": "Against Consent",
    "text": "Against Consent\nWe should abandon one idea that is deeply associated with privacy: consent. Typically, we think of consent as a sufficient condition for waiving privacy claims. That is, if someone agrees to share information that is theirs, then it’s OK to use their data. For example, if someone consents that the picture they uploaded to an online photo service is used by that service to train a generative AI model, then it is OK for the service to do so, since the customer consented to their photos being used in this way.\nOf course, consent only plays this role of making things OK that otherwise wouldn’t be OK if this consent is informed consent. And, crucially, in many cases in which we click “I agree” online, our consent is somewhat uninformed because of the pathology of privacy (we can’t process and know all the ways in which our data will be used, and we often lack a meaningful alternative to agreeing). As such, consent can only play a very limited role in privacy governance for practical reasons — because the bar for meaningful or informed consent is so high that, in practice, we can’t reach it.\nHowever, the externality of data — that personal information is often shared between individuals — is a further reason to abandon consent in privacy governance. Whereas the issue of informed consent is a practical limit to make consent worthwhile (we don’t have the time to make up our mind), the externality of data is an in-principle limit on using consent: the consent isn’t mine to give since the data isn’t only about me."
  },
  {
    "objectID": "ethics/ethics/pathology-of-privacy/index.html#footnotes",
    "href": "ethics/ethics/pathology-of-privacy/index.html#footnotes",
    "title": "The Pathology of Privacy",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee https://www.deceptive.design↩︎"
  }
]